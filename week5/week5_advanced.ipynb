{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week5 Advanced 심화과제\n",
    "\n",
    "- 논문요약\n",
    "\n",
    "    1) 필수 지시사항은 논문을 요약하는 것입니다.\n",
    "\n",
    "    2) 이를 조금 확장시켜, rag에 관한 여러가지 논문을 읽어와 rag에 관한 질문은 검색된 문서 기반으로 대답하고 일상 대화는 context에 상관없이 대답하는 챗봇을 만들 예정입니다.\n",
    "\n",
    "    3) 주어진 하나의 논문을 요약하는 것이 아닌 질의 응답을 통해 rag에 관한 개념 설명을 로그로 남기는 것을 목표로 합니다.\n",
    "\n",
    "- 요약 논문: arXiv에 게제된 rag에 관한 논문 9편\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 [My Code] Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"config.env\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 [My Code] Data Load\n",
    "\n",
    "- pdf를 불러올 땐 unstructuredfileloader를 사용합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1072/3843052403.py:8: LangChainDeprecationWarning: The class `UnstructuredFileLoader` was deprecated in LangChain 0.2.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-unstructured package and should be used instead. To use it run `pip install -U :class:`~langchain-unstructured` and import as `from :class:`~langchain_unstructured import UnstructuredLoader``.\n",
      "  documents = [UnstructuredFileLoader(file_path).load() for file_path in path.glob('*.pdf')]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# pdf를 불러올 땐 unstructuredFileLoader를 사용합니다\n",
    "path = Path('./data/essay')\n",
    "documents = [UnstructuredFileLoader(file_path).load() for file_path in path.glob('*.pdf')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 [My Code] TextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000개 길이를 기준으로 문서를 자르고 200개 길이만큼 겹치도록 문서를 구성합니다.\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splitted_docs = []\n",
    "for document in documents:\n",
    "    splitted_docs.extend(splitter.split_documents(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 [My Code] 문장 생성 모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 문장 생성기\n",
    "# 타 모델과의 비교도 가능하지만, 챗봇 구현시엔 해당 모델로 고정해두고 사용합니다. \n",
    "llm = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 [My Code] 임베딩 모델 선언\n",
    "\n",
    "- 여기선 CacheBackedEmbeddings을 사용하여, 인풋이 같을 때엔 다시 임베딩을 하지 않고 캐시된 정보를 사용하도록 수정합니다.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "\n",
    "# OpenAI Embeddings 초기화\n",
    "# https://platform.openai.com/docs/guides/embeddings\n",
    "# 임베딩 전용\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    api_key=openai.api_key,\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "# 이렇게 하면 동일한 파일에 대해서, 매번 임베딩하지 않습니다.\n",
    "cache_dir = LocalFileStore(\".cache/files\")\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "\n",
    "# Chroma 벡터 저장소 생성 및 로컬 저장 경로 지정\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splitted_docs,\n",
    "    embedding=cached_embeddings,\n",
    "    collection_name=\"my_db\"\n",
    ")\n",
    "\n",
    "\n",
    "# 유사 문서는 20개를 찾도록 합니다.\n",
    "# search_type에 similarity와 mmr이 보통 많이 사용되는데 여기선 similarity를 기준으로 하겠습니다. \n",
    "chroma_retriever = vectorstore.as_retriever(search_type='similarity', search_kwargs={'k': 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 [My Code] 프롬프트\n",
    "\n",
    "- LLM 교수와 학생의 질의응답 상황을 가정합니다.\n",
    "\n",
    "- rag와 관련 없는 질문은 context와 상관없이 대답하며 rag와 관련된 질문은 context 내에서 답변하도록 prompt를 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "# 반드시 프롬프트는 chatprompttemplate 객체에 넣어야 합니다.\n",
    "prompt_template = \"\"\"\n",
    "당신은 대학 LLM 교수입니다. 아래 지침에 따라 학생의 질문에 답해주세요.\n",
    "\n",
    "1. 문맥에서 답을 명확히 찾아내야 하며, **rag(Relevant Answer Generation)**에 관한 질문은 반드시 제공된 문맥에만 근거해서 답변해야 합니다.  \n",
    "   - rag에 관한 질문에서 문맥만으로 답을 찾을 수 없으면, \"아직 저도 부족해서 공부가 더 필요할 거 같아요. 같이 공부해봐요.\"라고 대답하세요.  \n",
    "2. rag와 관련 없는 일상적인 대화에는 자유롭게 답변하되, 친절하고 대화체로 작성하세요.  \n",
    "3. 모든 답변은 5문장을 넘지 않아야 하며, rag 관련 답변에는 반드시 \"잘 이해되었나요?\"라는 질문을 포함하세요.  \n",
    "4. 학생이 이해하기 쉽게 설명하며, 항상 친절한 교수님의 톤을 유지하세요.\n",
    "\n",
    "문맥:  \n",
    "{context}\n",
    "\n",
    "학생의 질문:  \n",
    "{question}\n",
    "\n",
    "답변:\n",
    "\"\"\"\n",
    "chat_prompt_template = ChatPromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 [My Code] 챗봇\n",
    "\n",
    "- 문맥은 5개만 기억하도록 설정합니다.\n",
    "\n",
    "- streamlit 등에선 따로 session이 존재하지만 파이썬 자체로 구현하기 위해 deque 데이터타입을 사용합니다.\n",
    "\n",
    "- 원래는, 기존의 모든 대화에서 query와 유사한 답변을 추출하여 세션으로 넘기는 로직 또한 포함되어야 하지만, 토이프로젝트에선 이를 생략하고 최근접 대화 5개만 세션으로 넘기도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM 수업 시작\n",
      "\n",
      "Student: 안녕하세요 교수님! 성함이 어떻게 되시나요?\n",
      "\n",
      "LLM Professor: 안녕하세요! 저는 이화여자대학교에서 사이버보안학과 교수로 재직 중인 배 호입니다. 반갑습니다! 궁금한 점이나 도움이 필요하신 부분이 있다면 언제든지 말씀해 주세요.\n",
      "\n",
      "Student: 배호 교수님이라고 부르면 될까요?\n",
      "\n",
      "LLM Professor: 네, 저를 배호 교수님이라고 부르시면 됩니다! 언제든지 궁금한 점이나 도움이 필요하신 부분이 있다면 편하게 말씀해 주세요. 함께 공부하고 논의하는 것을 항상 환영합니다. 반갑습니다!\n",
      "\n",
      "Student: 배호 교수님! 오늘 수업 주제는 무엇인가요?\n",
      "\n",
      "LLM Professor: 안녕하세요! 오늘 수업의 주제는 Retrieval-Augmented Generation(RAG)입니다. RAG는 최신 정보를 외부 데이터베이스에서 검색하여 대형 언어 모델(LLM)의 응답을 생성하는 기술로, LLM의 환각 현상을 최소화하고 정보의 정확성을 높이는 데 중요한 역할을 합니다. 수업에서는 RAG의 기본 개념, 구조, 그리고 최신 연구 동향에 대해 다룰 예정입니다. 궁금한 점이 있다면 언제든지 질문해 주세요!\n",
      "\n",
      "Student: rag가 어떻게 환각 현상을 최소화하지요?\n",
      "\n",
      "LLM Professor: RAG는 외부 데이터베이스에서 최신 정보를 검색하여 LLM의 응답을 생성하는 방식으로 작동합니다. 이를 통해 LLM이 학습한 정보에만 의존하지 않고, 외부에서 확인된 사실적인 데이터에 기반하여 답변할 수 있습니다. 결과적으로, 잘못된 정보나 환각 현상이 발생할 가능성을 줄일 수 있습니다. 또한, RAG는 검색된 정보의 정확성과 관련성을 평가하여 더욱 신뢰할 수 있는 응답을 생성하는 데 도움을 줍니다. 잘 이해되었나요?\n",
      "\n",
      "Student: rag와 데이터베이스의 관계에 대하여 설명해주세요\n",
      "\n",
      "LLM Professor: RAG는 외부 데이터베이스에서 정보를 검색하여 LLM의 응답을 생성하는 기술입니다. 이 과정에서 데이터베이스는 RAG의 핵심 역할을 하며, 최신 정보를 지속적으로 업데이트하여 사용자의 쿼리에 적합한 데이터를 제공합니다. RAG는 데이터베이스의 내용을 임베딩하여 벡터 형태로 저장하고, 사용자가 입력한 쿼리와의 유사도를 계산하여 관련 정보를 효율적으로 검색합니다. 이를 통해 LLM은 정확하고 신뢰할 수 있는 응답을 생성할 수 있습니다. 잘 이해되었나요?\n",
      "\n",
      "Student: rag의 방법론에 대하여 기술해주세요\n",
      "\n",
      "LLM Professor: 아직 저도 부족해서 공부가 더 필요할 거 같아요. 같이 공부해봐요.\n",
      "\n",
      "Student: 다시 답변해주세요\n",
      "\n",
      "LLM Professor: 아직 저도 부족해서 공부가 더 필요할 거 같아요. 같이 공부해봐요.\n",
      "\n",
      "Student: 방법론이란 단어에 대해 아시나요?\n",
      "\n",
      "LLM Professor: 네, \"방법론\"이라는 단어는 특정한 방법이나 절차, 혹은 접근 방식을 의미합니다. 주로 연구나 학문적인 분야에서 어떤 문제를 해결하기 위해 사용하는 체계적인 방법들을 설명할 때 자주 사용되죠. 예를 들어, RAG와 같은 기술의 경우, 정보 검색과 응답 생성을 위한 다양한 방법론이 적용될 수 있습니다. 방법론을 이해하는 것은 연구를 진행하는 데 매우 중요합니다. 더 궁금한 점이 있으면 언제든지 질문해 주세요!\n",
      "\n",
      "Student: 다시 답변해주세요!\n",
      "\n",
      "LLM Professor: 아직 저도 부족해서 공부가 더 필요할 거 같아요. 같이 공부해봐요!\n",
      "\n",
      "LLM Professor: 오늘 수업은 여기서 마치겠습니다.\n",
      "\n",
      "LLM 수업 끝\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(self, retriever, llm, prompt_template, history_limit=5):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.prompt_template = prompt_template\n",
    "        self.history = deque(maxlen=history_limit)\n",
    "\n",
    "    # docs: Document 객체의 list\n",
    "    @staticmethod\n",
    "    def _format_docs(docs):\n",
    "        return \"\\n\\n\".join(f\"Essay Source: {doc.metadata['source'].split('/')[-1]}, Content: {doc.page_content}\" for doc in docs)\n",
    "    \n",
    "    def create_prompt(self, user_message):\n",
    "        # 처음엔 context가 없어서 None으로 들어갑니다.\n",
    "        # 따라서 초반 서사가 쌓이기 전까진 retrieved_docs만 들어갑니다.\n",
    "        context = \"\\n\\n\".join([f\"Student: {msg['Student']}\\nLLM Professor: {msg['LLM Professor']}\" for msg in self.history])\n",
    "        retrieved_docs = self.retriever.invoke(user_message)\n",
    "\n",
    "        # 반환된 document를 전처리합니다.\n",
    "        retrieved_context = ChatBot._format_docs(retrieved_docs)\n",
    "        \n",
    "        return self.prompt_template.invoke({\n",
    "            \"context\": context + \"\\n\\n\" + retrieved_context,\n",
    "            \"question\": user_message\n",
    "        })\n",
    "\n",
    "    def chat(self, user_message):\n",
    "        user_message = user_message.lower()\n",
    "        user_prompt = self.create_prompt(user_message)\n",
    "        response = self.llm.invoke(user_prompt)\n",
    "        self.history.append({\"Student\": user_message, \"LLM Professor\": response})\n",
    "        return response\n",
    "    \n",
    "chatbot = ChatBot(retriever=chroma_retriever, llm=llm, prompt_template=chat_prompt_template)\n",
    "print(\"LLM 수업 시작\", end=\"\\n\\n\")\n",
    "while True:\n",
    "    user_input = input(\"Student: \")\n",
    "    if user_input.lower() in [\"종료\"]:\n",
    "        print(\"LLM Professor: 오늘 수업은 여기서 마치겠습니다.\", end=\"\\n\\n\")\n",
    "        print(\"LLM 수업 끝\", end=\"\\n\\n\")\n",
    "        break\n",
    "    response = chatbot.chat(user_input)\n",
    "    print(f\"Student: {user_input}\", end=\"\\n\\n\")\n",
    "    print(f\"LLM Professor: {response.content}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 [My Code] Conclusion\n",
    "\n",
    "1) 간단한 프롬프트와 클래스 선언 만으로 기본적인 챗봇이 구현되는 것을 확인하였습니다.\n",
    "\n",
    "2) 적절한 질의응답을 통해 rag에 관하여 context 바깥의 질문을 던지면 답변을 안하는 것을 확인하였습니다. 이는 언어 모델의 Hallucinations를 방지하는 효과를 검증한 것입니다.\n",
    "\n",
    "3) 일상적인 대화에 대해서는 gpt가 스스로 답변을 하는 것 또한 확인하였습니다. 이는 대화의 주제를 기반으로 모델 스스로 context 내의 주제인지 아닌지를 판단하고 있음을 확인한 것입니다.\n",
    "\n",
    "4) 더 해볼 과제로는, 문장 생성과 임베딩 모델을 로컬 모델로 바꾸는 것과, 챗봇 구현 과정에서 세션을 구성하는 로직을 정교화하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
