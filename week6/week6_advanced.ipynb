{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_950/3175900557.py:15: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ Title: How Do Large Language Models Acquire Factual Knowledge During Pretraining?\n",
      "ğŸ“ Authors: Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo\n",
      "ğŸ“… Submitted: 2024-06-17 17:54:40+00:00\n",
      "ğŸ”— PDF Link: http://arxiv.org/pdf/2406.11813v3\n",
      "ğŸ“ Abstract:\n",
      "Despite the recent observation that large language models (LLMs) can store\n",
      "substantial factual knowledge, there is a limited understanding of the\n",
      "mechanisms of how they acquire factual knowledge through pretraining. This work\n",
      "addresses this gap by studying how LLMs acquire factual knowledge during\n",
      "pretraining. The findings reveal several important insights into the dynamics\n",
      "of factual knowledge acquisition during pretraining. First, counterintuitively,\n",
      "we observe that pretraining on more data shows no significant improvement in\n",
      "the model's capability to acquire and maintain factual knowledge. Next, there\n",
      "is a power-law relationship between training steps and forgetting of\n",
      "memorization and generalization of factual knowledge, and LLMs trained with\n",
      "duplicated training data exhibit faster forgetting. Third, training LLMs with\n",
      "larger batch sizes can enhance the models' robustness to forgetting. Overall,\n",
      "our observations suggest that factual knowledge acquisition in LLM pretraining\n",
      "occurs by progressively increasing the probability of factual knowledge\n",
      "presented in the pretraining data at each step. However, this increase is\n",
      "diluted by subsequent forgetting. Based on this interpretation, we demonstrate\n",
      "that we can provide plausible explanations for recently observed behaviors of\n",
      "LLMs, such as the poor performance of LLMs on long-tail knowledge and the\n",
      "benefits of deduplicating the pretraining corpus.\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import requests\n",
    "from pathlib import Path\n",
    "## ìš”ì•½ ë° ë¦¬ë·°í•˜ë ¤ëŠ” ë…¼ë¬¸ì„ ì•„ì¹´ì´ë¸Œì—ì„œ idë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. \n",
    "## ì´í›„ ë‹¤ìš´ì„ ë°›ìŠµë‹ˆë‹¤. \n",
    "def download_arxiv_pdf(pdf_url, save_path):\n",
    "    response = requests.get(pdf_url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                file.write(chunk)\n",
    "\n",
    "arxiv_id = \"2406.11813\"\n",
    "search = arxiv.Search(id_list=[arxiv_id])\n",
    "for result in search.results():\n",
    "    break\n",
    "save_path = Path('references') / f\"0-{result.title[:15]}.pdf\"\n",
    "pdf_url = result.pdf_url\n",
    "download_arxiv_pdf(pdf_url, save_path)\n",
    "\n",
    "print(f\"ğŸ“Œ Title: {result.title}\")\n",
    "print(f\"ğŸ“ Authors: {', '.join([author.name for author in result.authors])}\")\n",
    "print(f\"ğŸ“… Submitted: {result.published}\")\n",
    "print(f\"ğŸ”— PDF Link: {result.pdf_url}\")\n",
    "print(f\"ğŸ“ Abstract:\\n{result.summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "# ì—¬ê¸°ëŠ” ì†ìˆ˜ ì •ë¦¬í•´ì¤˜ì•¼ í•©ë‹ˆë‹¤.\n",
    "# ë…¼ë¬¸ì„ section ë³„ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "loader = UnstructuredPDFLoader(save_path)\n",
    "documents = loader.load()\n",
    "output = {\n",
    "    \"Title\":result.title,\n",
    "    \"Authors\":'\\n'.join([author.name for author in result.authors]),\n",
    "    \"Submitted\":str(result.published),\n",
    "    \"Abstract\":result.summary,\n",
    "    # ì•„ë˜ëŠ” ë…¼ë¬¸ì˜ reference ì¸ìš© íšŸìˆ˜ë¥¼ countí•˜ê¸° ìœ„í•œ ê¸°ì´ˆ ì‘ì—…ì…ë‹ˆë‹¤.\n",
    "    \"Introduction\":documents[0].page_content.split(\"Introduction\\n\\n\")[-1].split(\"Related Work\\n\\n\")[0],\n",
    "    \"Related Work\":documents[0].page_content.split(\"Related Work\\n\\n\")[-1].split(\"Experimental Setup\\n\\n\")[0],\n",
    "    \"Experimental Setup\":documents[0].page_content.split(\"Experimental Setup\\n\\n\")[-1].split(\"Results\\n\\n\")[0],\n",
    "    \"Results\":documents[0].page_content.split(\"Results\\n\\n\")[-1].split(\"Discussion and Conclusions\\n\\n\")[0],\n",
    "    \"Discussion and Conclusions\":documents[0].page_content.split(\"Discussion and Conclusions\\n\\n\")[-1].split(\"References\\n\\n\")[0],\n",
    "    \"References\":documents[0].page_content.split(\"References\\n\\n\")[-1].split(\"Appendix\\n\\n\")[0],\n",
    "    \"Appendix\":documents[0].page_content.split(\"Appendix\\n\\n\")[-1],\n",
    "}\n",
    "reference_count_keys = [\"Introduction\", \"Related Work\", \"Experimental Setup\", \"Results\", \"Discussion and Conclusions\"]\n",
    "content_keys = [\"Title\",\"Authors\",\"Submitted\",\"Abstract\",\"Introduction\", \"Related Work\", \"Experimental Setup\", \"Results\", \"Discussion and Conclusions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\\n\" ì‚¬ì´ì˜ ë¬¸ìê°€ threshold ë¯¸ë§Œì´ë©´ ì œê±°í•©ë‹ˆë‹¤.\n",
    "threshold = 25\n",
    "\n",
    "for key in reference_count_keys:\n",
    "    result = []\n",
    "    for text in output[key].split(\"\\n\"):\n",
    "        if len(text) >= threshold:\n",
    "            result.append(text)\n",
    "    output[key] = \"\\n\".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "# .envê°€ ì•„ë‹Œ config.envì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv(dotenv_path=\"config.env\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI()\n",
    "\n",
    "def message_to_openai(user, model = 'gpt-4o-mini'):\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        store=True,\n",
    "        messages=[{\"role\": \"user\", \"content\": user}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response\n",
    "\n",
    "user_prompt_template = \"\"\"\n",
    "ë…¼ë¬¸: {essay}\n",
    "\n",
    "ë‹¤ìŒ ë…¼ë¬¸ì˜ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”. í•­ëª©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "## í•­ëª© ì •ë³´\n",
    "1. ê¸°ë³¸ ì •ë³´: ë…¼ë¬¸ì˜ ì œëª©ê³¼ ì €ì, ëª¨ë‘ ì˜ë¬¸ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤. \n",
    "2. ì—°êµ¬ ëª©ì : ì´ ë…¼ë¬¸ì´ ê°€ì§„ ë¬¸ì œì˜ì‹ê³¼ ì´ì— ëŒ€í•œ ì„¤ëª…ì„ ì‘ì„±í•©ë‹ˆë‹¤. ì´ë•Œ ë¬¸ì œì˜ì‹ì€ ëª…ì‚¬í˜•ìœ¼ë¡œ ëë‚´ê³  ì„¤ëª…ì€ 300ìì—ì„œ 500ìë¡œ ì„œìˆ í•©ë‹ˆë‹¤. \n",
    "3. ì—°êµ¬ ë°©ë²•: í•´ë‹¹ ë…¼ë¬¸ì´ ì—°êµ¬í•œ ì‹¤í—˜ ë°©ë²•ê³¼ ë°ì´í„°, ëª¨ë¸, ë¶„ì„ ë°©ë²• ë“±ì„ ì‘ì„±í•©ë‹ˆë‹¤. ì—°êµ¬ ë°©ë²•ì€ ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ìœ¼ë¡œ ê¸°ìˆ í•©ë‹ˆë‹¤.\n",
    "4. ì£¼ìš” ê²°ê³¼: ë…¼ë¬¸ì˜ ì—°êµ¬ ì„±ê³¼ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤. ì´ë•Œ ì—°êµ¬ì˜ í•µì‹¬ì ì¸ ë°œê²¬ê³¼ ë…¼ë¬¸ì˜ ì£¼ìš” ê¸°ì—¬ë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤.\n",
    "5. ê²°ë¡  ë° ì‹œì‚¬ì : ë…¼ë¬¸ì˜ ê²°ë¡ ê³¼ ì—°êµ¬ì˜ ì˜ë¯¸ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤. ë˜í•œ ì—°êµ¬ì˜ í•œê³„ì ê³¼ í–¥í›„ ì—°êµ¬ ë°©í–¥ì„ í¬í•¨í•˜ì—¬ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì‘ë‹µ ì˜ˆì‹œ\n",
    "\n",
    "### 1. ê¸°ë³¸ ì •ë³´\n",
    "1) ì œëª©: <<ë…¼ë¬¸ ì œëª©(ì˜ë¬¸)>>\n",
    "2) ì €ì: <<ë…¼ë¬¸ ì €ì(ì˜ë¬¸)>>\n",
    "\n",
    "### 2. ì—°êµ¬ ëª©ì \n",
    "1) ë¬¸ì œì˜ì‹: <<ë¬¸ì œ ì˜ì‹ì„ ëª…ì‚¬í˜•ìœ¼ë¡œ êµ¬ì„±>>\n",
    "2) ì„¤ëª…: <<ë¬¸ì œ ì˜ì‹ì— ëŒ€í•œ ì„¤ëª…ì„ 300ìì—ì„œ 500ìë¡œ ì„œìˆ >>\n",
    "\n",
    "### 3. ì—°êµ¬ ë°©ë²•\n",
    "1) ì‹¤í—˜ ë°©ë²•: <<ì—°êµ¬ì—ì„œ ì‚¬ìš©í•œ ì‹¤í—˜ ë°©ë²•ì„ ê¸°ìˆ >>\n",
    "2) ë°ì´í„°: <<ì‚¬ìš©ëœ ë°ì´í„°ì˜ ì¶œì²˜ ë° íŠ¹ì„±ì„ ì„¤ëª…>>\n",
    "3) ëª¨ë¸ ë° ë¶„ì„ ë°©ë²•: <<ì ìš©ëœ ëª¨ë¸ê³¼ ë¶„ì„ ê¸°ë²•ì„ ì„¤ëª…>>\n",
    "\n",
    "### 4. ì£¼ìš” ê²°ê³¼\n",
    "1) ì—°êµ¬ì˜ ì£¼ìš” ë°œê²¬: <<ë…¼ë¬¸ì˜ í•µì‹¬ ê²°ê³¼ë¥¼ ìš”ì•½>>\n",
    "2) ê¸°ì—¬ ë° ì„±ê³¼: <<ì—°êµ¬ê°€ ê¸°ì¡´ ì—°êµ¬ ëŒ€ë¹„ ê¸°ì—¬í•œ ì ì„ ì„¤ëª…>>\n",
    "\n",
    "### 5. ê²°ë¡  ë° ì‹œì‚¬ì \n",
    "1) ê²°ë¡ : <<ì—°êµ¬ì˜ ì£¼ìš” ê²°ë¡ ì„ ìš”ì•½>>\n",
    "2) ì‹œì‚¬ì : <<ì—°êµ¬ê°€ ì‹œì‚¬í•˜ëŠ” ë°”ì™€ í™œìš© ê°€ëŠ¥ì„±ì„ ì„¤ëª…>>\n",
    "3) ì—°êµ¬ì˜ í•œê³„: <<ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰í•œ ì—°êµ¬ì˜ í•œê³„ë¥¼ ê¸°ìˆ >>\n",
    "4) í–¥í›„ ì—°êµ¬ ë°©í–¥: <<ì¶”ê°€ ì—°êµ¬ê°€ í•„ìš”í•œ ë¶€ë¶„ì„ ì„¤ëª…>>\n",
    "\n",
    "ê°€ëŠ¥í•œ í•œ ë…¼ë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ ìœ ì§€í•˜ë©´ì„œ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay = \"\\n\\n\".join([output[key] for key in content_keys])\n",
    "user_message = user_prompt_template.format(essay=essay)\n",
    "response = message_to_openai(user_message)\n",
    "output['Basic_summary'] = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 1. ê¸°ë³¸ ì •ë³´\n",
      "1) ì œëª©: How Do Large Language Models Acquire Factual Knowledge During Pretraining?\n",
      "2) ì €ì: Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo\n",
      "\n",
      "### 2. ì—°êµ¬ ëª©ì \n",
      "1) ë¬¸ì œì˜ì‹: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì‚¬ì‹¤ì  ì§€ì‹ ìŠµë“ ë©”ì»¤ë‹ˆì¦˜\n",
      "2) ì„¤ëª…: ìµœê·¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ìƒë‹¹í•œ ì‚¬ì‹¤ì  ì§€ì‹ì„ ì €ì¥í•  ìˆ˜ ìˆë‹¤ëŠ” ê´€ì°°ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì´ë“¤ì´ ì‚¬ì „ í›ˆë ¨ ì¤‘ ì‚¬ì‹¤ì  ì§€ì‹ì„ ì–´ë–»ê²Œ ìŠµë“í•˜ëŠ”ì§€ì— ëŒ€í•œ ì´í•´ëŠ” ì œí•œì ì´ë‹¤. ë³¸ ì—°êµ¬ëŠ” LLMì˜ ì‚¬ì‹¤ì  ì§€ì‹ ìŠµë“ ê³¼ì •ì„ ë¶„ì„í•˜ì—¬, í›ˆë ¨ ë°ì´í„°ì˜ ì–‘, í›ˆë ¨ ë‹¨ê³„, ëª¨ë¸ í¬ê¸°, ë°°ì¹˜ í¬ê¸° ë“±ì˜ ë‹¤ì–‘í•œ ì¡°ê±´ì´ ì§€ì‹ ìŠµë“ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì¡°ì‚¬í•œë‹¤. ì´ë¥¼ í†µí•´ LLMì˜ í›ˆë ¨ ë™ì—­í•™ì„ ì´í•´í•˜ê³ , ì§€ì‹ ìŠµë“ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ê·œëª…í•˜ê³ ì í•œë‹¤.\n",
      "\n",
      "### 3. ì—°êµ¬ ë°©ë²•\n",
      "1) ì‹¤í—˜ ë°©ë²•: ì—°êµ¬ì§„ì€ LLMì˜ ì¤‘ê°„ ì‚¬ì „ í›ˆë ¨ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬, ì´ì „ì— ì ‘í•˜ì§€ ì•Šì€ ëª©í‘œ ì§€ì‹ì„ ì£¼ì…í•˜ê³ , ë‹¤ì–‘í•œ ì¡°ê±´ì—ì„œ ì‚¬ì‹¤ì  ì§€ì‹ì˜ ìŠµë“ ê³¼ì •ì„ ëª¨ë‹ˆí„°ë§í•˜ì˜€ë‹¤. \n",
      "2) ë°ì´í„°: FICTIONAL KNOWLEDGE ë°ì´í„°ì…‹ì„ êµ¬ì„±í•˜ì—¬, í—ˆêµ¬ì ì´ì§€ë§Œ í˜„ì‹¤ì ì¸ ê°œì²´ì— ëŒ€í•œ ì„¤ëª…ì„ í¬í•¨í•œ ë¬¸ì¥ì„ ì£¼ì…í•˜ì˜€ë‹¤. ì´ ë°ì´í„°ì…‹ì€ LLMì´ í›ˆë ¨ ì¤‘ì— ì ‘í•˜ì§€ ì•Šì€ ì§€ì‹ì„ í¬í•¨í•œë‹¤.\n",
      "3) ëª¨ë¸ ë° ë¶„ì„ ë°©ë²•: OLMo ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬, ì£¼ì…ëœ ì§€ì‹ì— ëŒ€í•œ ë¡œê·¸ í™•ë¥ ì„ í‰ê°€í•˜ê³ , ì§€ì‹ ìŠµë“ì˜ íš¨ê³¼ì„± ë° ìœ ì§€ ê°€ëŠ¥ì„±ì„ ì¸¡ì •í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ì„ ì •ì˜í•˜ì˜€ë‹¤.\n",
      "\n",
      "### 4. ì£¼ìš” ê²°ê³¼\n",
      "1) ì—°êµ¬ì˜ ì£¼ìš” ë°œê²¬: LLMì€ ì‚¬ì‹¤ì  ì§€ì‹ì„ ìŠµë“í•  ë•Œ, ë¯¸ì„¸í•œ í™•ë¥ ì˜ ëˆ„ì ì„ í†µí•´ ì´ë£¨ì–´ì§€ë©°, í›ˆë ¨ ì¤‘ ì§€ì‹ì´ ì£¼ì–´ì§€ì§€ ì•Šìœ¼ë©´ ìŠì–´ë²„ë¦¬ëŠ” ê²½í–¥ì´ ìˆë‹¤. ë˜í•œ, ëª¨ë¸ í¬ê¸°ê°€ í´ìˆ˜ë¡ ì§€ì‹ ìŠµë“ì˜ íš¨ê³¼ì„±ì´ ë†’ì•„ì§€ì§€ë§Œ, í›ˆë ¨ ë°ì´í„°ì˜ ì–‘ì´ ë§ì•„ì ¸ë„ íš¨ê³¼ì„±ì€ í¬ê²Œ ê°œì„ ë˜ì§€ ì•ŠëŠ”ë‹¤.\n",
      "2) ê¸°ì—¬ ë° ì„±ê³¼: ë³¸ ì—°êµ¬ëŠ” LLMì˜ ì‚¬ì‹¤ì  ì§€ì‹ ìŠµë“ ë™ì—­í•™ì„ ì„¸ë°€í•˜ê²Œ ë¶„ì„í•˜ê³ , ë°ì´í„° ì¤‘ë³µ ì œê±°ì™€ ë°°ì¹˜ í¬ê¸° ì¦ê°€ê°€ ì§€ì‹ ìŠµë“ì— ê¸ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ì ì„ ë°í˜€ë‚´ì–´, LLMì˜ í›ˆë ¨ ë°©ë²•ë¡ ì— ëŒ€í•œ ìƒˆë¡œìš´ í†µì°°ì„ ì œê³µí•œë‹¤.\n",
      "\n",
      "### 5. ê²°ë¡  ë° ì‹œì‚¬ì \n",
      "1) ê²°ë¡ : LLMì˜ ì‚¬ì‹¤ì  ì§€ì‹ ìŠµë“ì€ ë¯¸ì„¸í•œ í™•ë¥ ì˜ ëˆ„ì ì„ í†µí•´ ì´ë£¨ì–´ì§€ë©°, ìŠì–´ë²„ë¦¼ í˜„ìƒê³¼ì˜ ê´€ê³„ê°€ ì¤‘ìš”í•˜ë‹¤. \n",
      "2) ì‹œì‚¬ì : ì—°êµ¬ ê²°ê³¼ëŠ” LLMì˜ í›ˆë ¨ ë°ì´í„° êµ¬ì„± ë° í›ˆë ¨ ë°©ë²•ì— ëŒ€í•œ ì „ëµì  ì ‘ê·¼ì„ ì œì•ˆí•˜ë©°, LLMì˜ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í•  ìˆ˜ ìˆë‹¤.\n",
      "3) ì—°êµ¬ì˜ í•œê³„: ë³¸ ì—°êµ¬ëŠ” íŠ¹ì • ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì— êµ­í•œë˜ì–´ ìˆìœ¼ë©°, ë‹¤ì–‘í•œ LLM ì•„í‚¤í…ì²˜ì— ëŒ€í•œ ì¼ë°˜í™” ê°€ëŠ¥ì„±ì— ëŒ€í•œ ê²€ì¦ì´ í•„ìš”í•˜ë‹¤.\n",
      "4) í–¥í›„ ì—°êµ¬ ë°©í–¥: LLMì˜ ì§€ì‹ ìŠµë“ ë©”ì»¤ë‹ˆì¦˜ì„ ë”ìš± ê¹Šì´ ì´í•´í•˜ê¸° ìœ„í•´, ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì§€ì‹ê³¼ í›ˆë ¨ ì¡°ê±´ì„ íƒìƒ‰í•˜ëŠ” ì¶”ê°€ ì—°êµ¬ê°€ í•„ìš”í•˜ë‹¤.\n"
     ]
    }
   ],
   "source": [
    "print(output['Basic_summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# ì„¹ì…˜ ë³„ë¡œ 2000ê¸€ì ë‹¨ìœ„ë¡œ ìª¼ê°­ë‹ˆë‹¤.\n",
    "# ìª¼ê°œëŠ” ëª©ì ì€ ì¸ìš© ë¶€ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.\n",
    "def preprocess(text, title, key, chunk_size=2000, chunk_overlap=0):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap  \n",
    "    )\n",
    "\n",
    "    documents = text_splitter.create_documents([text])\n",
    "\n",
    "    # ê° ë¬¸ì„œì— ë©”íƒ€ë°ì´í„° ì¶”ê°€\n",
    "    for doc in documents:\n",
    "        doc.metadata = {\"Title\": title, \"Key\": key} \n",
    "\n",
    "    return documents\n",
    "\n",
    "for key in reference_count_keys:\n",
    "    output[key] = preprocess(output[key], output['Title'], key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message_template = \"\"\"\n",
    "ë‹¤ìŒì€ ë…¼ë¬¸ì—ì„œ reference ë¶€ë¶„ë§Œ ë°œì·Œí•œ ê²ƒì…ë‹ˆë‹¤. \n",
    "ì´ë¥¼ ë³´ê³  ë…¼ë¬¸ì˜ ì œëª©ê³¼ ì €ìë¥¼ ì¶œë ¥ í˜•ì‹ì— ë”°ë¼ JSON í˜•íƒœë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "References:\n",
    "{references}\n",
    "\n",
    "ì¶œë ¥ í˜•ì‹:\n",
    "```json\n",
    "{{\n",
    "1 : {{\"Title\":<<ë…¼ë¬¸ì˜ ì œëª©1>>, \"Authors\":<<ë…¼ë¬¸ì˜ ì €ì1>>}},\n",
    "2 : {{\"Title\":<<ë…¼ë¬¸ì˜ ì œëª©2>>, \"Authors\":<<ë…¼ë¬¸ì˜ ì €ì2>>}},\n",
    "3 : {{\"Title\":<<ë…¼ë¬¸ì˜ ì œëª©3>>, \"Authors\":<<ë…¼ë¬¸ì˜ ì €ì3>>}}\n",
    "}}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "user_message = user_message_template.format(references=output['References'])\n",
    "response = message_to_openai(user_message)\n",
    "\n",
    "text = response.choices[0].message.content\n",
    "text = re.sub(\"```json\",\"\",text)\n",
    "text = re.sub(\"```\",\"\",text)\n",
    "json_data = json.loads(text)\n",
    "\n",
    "reference_dict = {}\n",
    "for key, dict_data in json_data.items():\n",
    "    dict_data['Counter'] = 0\n",
    "    dict_data['Context'] = []\n",
    "    reference_dict[key] = dict_data\n",
    "\n",
    "output['References'] = deepcopy(reference_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message_template_dict = {\n",
    "    '0': \"\"\"\n",
    "ë‹¤ìŒì€ ë…¼ë¬¸ì˜ ì¼ë¶€ì™€ ì°¸ê³ ë¬¸í—Œ ëª©ë¡ì„ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤. \n",
    "ë…¼ë¬¸ ë³¸ë¬¸ì—ì„œ ì°¸ê³ ë¬¸í—Œì´ ì¸ìš©ë˜ì—ˆë‹¤ë©´, í•´ë‹¹ ì°¸ê³ ë¬¸í—Œì˜ \"Counter\"ì— ì¸ìš© íšŸìˆ˜ë¥¼ ê¸°ë¡í•˜ê³ ,  \n",
    "\"Context\"ì—ëŠ” í•´ë‹¹ ì°¸ê³ ë¬¸í—Œì´ ì¸ìš©ëœ ë¶€ë¶„ì„ **300ì ì´ë‚´**ë¡œ ë°œì·Œí•´ì„œ ì €ì¥í•˜ì„¸ìš”.  \n",
    "**ì¸ìš©ë˜ì§€ ì•Šì€ ì°¸ê³ ë¬¸í—Œì€ ì¶œë ¥í•˜ì§€ ë§ê³ , ì¸ìš©ëœ ì°¸ê³ ë¬¸í—Œë§Œ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.**\n",
    "\n",
    "**ì¡°ê±´**\n",
    "- ì¸ìš© í‘œì‹œ: ë…¼ë¬¸ ë³¸ë¬¸ì—ì„œ ì°¸ê³ ë¬¸í—Œì´ **[ìˆ«ì]** í˜•ì‹ (`[5]`, `[27]` ë“±)ìœ¼ë¡œ ì¸ìš©ëœ ê²½ìš°ë§Œ ì¶”ì¶œí•˜ì„¸ìš”.\n",
    "- ì°¸ê³ ë¬¸í—Œì´ ì—¬ëŸ¬ ë²ˆ ì¸ìš©ë˜ì—ˆì„ ê²½ìš°, ê° ì¸ìš© ë¶€ë¶„ì„ `\"Context\"` ë°°ì—´ì— ì €ì¥í•˜ì„¸ìš”.\n",
    "- `\"Context\"`ì— í¬í•¨ë˜ëŠ” ì¸ìš©ëœ ë¬¸ì¥ì€ **ìµœëŒ€ 300ì ì´ë‚´**ë¡œ ìœ ì§€í•˜ì„¸ìš”.\n",
    "\n",
    "**ì¶œë ¥ í˜•ì‹**\n",
    "```json\n",
    "{{\n",
    "    \"27\": {{\n",
    "        \"Title\": \"ì°¸ê³ ë¬¸í—Œ ì œëª©1\",\n",
    "        \"Counter\": 3,\n",
    "        \"Context\": [\"ì¸ìš©ëœ ë¶€ë¶„1\", \"ì¸ìš©ëœ ë¶€ë¶„2\", \"ì¸ìš©ëœ ë¶€ë¶„3\"]\n",
    "    }},\n",
    "    \"5\": {{\n",
    "        \"Title\": \"ì°¸ê³ ë¬¸í—Œ ì œëª©2\",\n",
    "        \"Counter\": 1,\n",
    "        \"Context\": [\"ì¸ìš©ëœ ë¶€ë¶„1\"]\n",
    "    }}\n",
    "}}\n",
    "```\n",
    "\n",
    "**References**\n",
    "{references}\n",
    "\n",
    "**Essay**\n",
    "{essay}\n",
    "\n",
    "\"\"\",\n",
    "    \"1\" : \"\"\"\n",
    "ë…¼ë¬¸ì˜ ì¼ë¶€ì™€ ì°¸ê³ ë¬¸í—Œ ëª©ë¡ì´ ì œê³µë˜ì—ˆìŠµë‹ˆë‹¤. \n",
    "ë…¼ë¬¸ ë³¸ë¬¸ì—ì„œ íŠ¹ì • ì°¸ê³ ë¬¸í—Œì´ ì¸ìš©ë˜ì—ˆë‹¤ë©´, í•´ë‹¹ ì°¸ê³ ë¬¸í—Œì˜ **ì¸ìš© íšŸìˆ˜**ë¥¼ `\"Counter\"`ì— ê¸°ë¡í•˜ê³ , \n",
    "**ì¸ìš©ëœ ë¬¸ì¥**ì„ `\"Context\"`ì— ìµœëŒ€ 300ì ì´ë‚´ë¡œ ë°œì·Œí•˜ì—¬ ì €ì¥í•˜ì„¸ìš”.  \n",
    "**í•œ ë²ˆë„ ì¸ìš©ë˜ì§€ ì•Šì€ ì°¸ê³ ë¬¸í—Œì€ ì¶œë ¥ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.**\n",
    "\n",
    "## ì¡°ê±´\n",
    "- ì¸ìš©í‘œì‹œ: ë³¸ë¬¸ì—ì„œ ì°¸ê³ ë¬¸í—Œì´ **[ìˆ«ì]** (`[10]`, `[3]` ë“±) í˜•ì‹ìœ¼ë¡œ ì¸ìš©ëœ ê²½ìš°ë§Œ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "- ë™ì¼í•œ ì°¸ê³ ë¬¸í—Œì´ ì—¬ëŸ¬ ë²ˆ ì¸ìš©ë˜ë©´ `\"Context\"` ë°°ì—´ì— ëª¨ë“  ì¸ìš© ë¶€ë¶„ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "- `\"Context\"`ì˜ ê°œë³„ í•­ëª©ì€ **ìµœëŒ€ 300ì ì´í•˜**ë¥¼ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì¶œë ¥ ì˜ˆì‹œ\n",
    "```json\n",
    "{{\n",
    "    \"10\": {{\n",
    "        \"Title\": \"ì°¸ê³ ë¬¸í—Œ ì œëª©1\",\n",
    "        \"Counter\": 2,\n",
    "        \"Context\": [\"ì²« ë²ˆì§¸ ì¸ìš© ë¬¸ì¥\", \"ë‘ ë²ˆì§¸ ì¸ìš© ë¬¸ì¥\"]\n",
    "    }},\n",
    "    \"3\": {{\n",
    "        \"Title\": \"ì°¸ê³ ë¬¸í—Œ ì œëª©2\",\n",
    "        \"Counter\": 1,\n",
    "        \"Context\": [\"ì¸ìš©ëœ ë¬¸ì¥\"]\n",
    "    }}\n",
    "}}\n",
    "```\n",
    "\n",
    "## References\n",
    "{references}\n",
    "\n",
    "## Essay\n",
    "{essay}\n",
    "\"\"\",\n",
    "\n",
    "    \"2\": \"\"\"\n",
    "ë…¼ë¬¸ì˜ ë³¸ë¬¸ê³¼ ì°¸ê³ ë¬¸í—Œ ëª©ë¡ì„ ì •ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.  \n",
    "ë…¼ë¬¸ì—ì„œ íŠ¹ì • ì°¸ê³ ë¬¸í—Œì´ ì¸ìš©ë˜ì—ˆì„ ê²½ìš°,\n",
    "í•´ë‹¹ ì°¸ê³ ë¬¸í—Œì˜ **ì¸ìš© íšŸìˆ˜(`\"Counter\"`)**ì™€ **ì¸ìš© ë¬¸ì¥(`\"Context\"`)**ì„ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.  \n",
    "**ë³¸ë¬¸ì—ì„œ í•œ ë²ˆë„ ì¸ìš©ë˜ì§€ ì•Šì€ ì°¸ê³ ë¬¸í—Œì€ ì¶œë ¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.**\n",
    "\n",
    "## ì¡°ê±´\n",
    "- ì¸ìš© í‘œì‹œ: ì°¸ê³ ë¬¸í—Œì´ ë³¸ë¬¸ì—ì„œ **[ìˆ«ì]** (`[7]`, `[19]` ë“±) í˜•ì‹ìœ¼ë¡œ ë“±ì¥í•˜ëŠ” ê²½ìš°ë§Œ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "- ë™ì¼í•œ ì°¸ê³ ë¬¸í—Œì´ ì—¬ëŸ¬ ë²ˆ ì¸ìš©ë˜ì—ˆì„ ê²½ìš°, `\"Context\"` ë°°ì—´ì— ê° ì¸ìš© ë¬¸ì¥ì„ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "- `\"Context\"`ì˜ ê° ë¬¸ì¥ì€ **300ì ì´ë‚´**ë¡œ ì œí•œí•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì¶œë ¥ ì˜ˆì‹œ\n",
    "```json\n",
    "{{\n",
    "    \"7\": {{\n",
    "        \"Title\": \"ì°¸ê³ ë¬¸í—Œ ì œëª©1\",\n",
    "        \"Counter\": 3,\n",
    "        \"Context\": [\"ì²« ë²ˆì§¸ ì¸ìš© ë¬¸ì¥\", \"ë‘ ë²ˆì§¸ ì¸ìš© ë¬¸ì¥\", \"ì„¸ ë²ˆì§¸ ì¸ìš© ë¬¸ì¥\"]\n",
    "    }},\n",
    "    \"19\": {{\n",
    "        \"Title\": \"ì°¸ê³ ë¬¸í—Œ ì œëª©2\",\n",
    "        \"Counter\": 1,\n",
    "        \"Context\": [\"ì¸ìš©ëœ ë¬¸ì¥\"]\n",
    "    }}\n",
    "}}\n",
    "```\n",
    "\n",
    "## References\n",
    "{references}\n",
    "\n",
    "## Essay\n",
    "{essay}\n",
    "\"\"\",\n",
    "\n",
    "    \"3\": \"\"\"\n",
    "ë…¼ë¬¸ì˜ ë³¸ë¬¸ê³¼ ì°¸ê³ ë¬¸í—Œ ëª©ë¡ì´ ì£¼ì–´ì¡ŒìŠµë‹ˆë‹¤.  \n",
    "ë…¼ë¬¸ì—ì„œ ì°¸ê³ ë¬¸í—Œì´ ì¸ìš©ë˜ì—ˆë‹¤ë©´,\n",
    "í•´ë‹¹ ì°¸ê³ ë¬¸í—Œì˜ ì¸ìš© íšŸìˆ˜ë¥¼ `\"Counter\"`ì— ê¸°ë¡í•˜ê³ ,  \n",
    "ì¸ìš©ëœ ë¬¸ì¥ì„ 300ì ì´ë‚´ë¡œ ì •ë¦¬í•˜ì—¬ `\"Context\"` ë°°ì—´ì— ì €ì¥í•˜ì„¸ìš”.  \n",
    "**ì¸ìš©ë˜ì§€ ì•Šì€ ì°¸ê³ ë¬¸í—Œì€ ì¶œë ¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.**\n",
    "\n",
    "## ì¡°ê±´\n",
    "- ì¸ìš© í‘œì‹œ: ì°¸ê³ ë¬¸í—Œì´ ë³¸ë¬¸ì—ì„œ **[ìˆ«ì]** (`[6]`, `[22]` ë“±) í˜•ì‹ìœ¼ë¡œ ë“±ì¥í•œ ê²½ìš°ë§Œ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "- ë™ì¼í•œ ì°¸ê³ ë¬¸í—Œì´ ì—¬ëŸ¬ ë²ˆ ì¸ìš©ëœ ê²½ìš° `\"Context\"` ë°°ì—´ì— ëª¨ë“  ì¸ìš© ë¬¸ì¥ì„ ê¸°ë¡í•©ë‹ˆë‹¤.\n",
    "- `\"Context\"`ì˜ ê°œë³„ í•­ëª©ì€ **ìµœëŒ€ 300ì ì´í•˜**ë¡œ ì œí•œí•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì¶œë ¥ ì˜ˆì‹œ\n",
    "```json\n",
    "{{\n",
    "    \"6\": {{\n",
    "        \"Title\": \"ì°¸ê³ ë¬¸í—Œ ì œëª©1\",\n",
    "        \"Counter\": 4,\n",
    "        \"Context\": [\"ì²« ë²ˆì§¸ ì¸ìš© ë¬¸ì¥\", \"ë‘ ë²ˆì§¸ ì¸ìš© ë¬¸ì¥\", \"ì„¸ ë²ˆì§¸ ì¸ìš© ë¬¸ì¥\", \"ë„¤ ë²ˆì§¸ ì¸ìš© ë¬¸ì¥\"]\n",
    "    }},\n",
    "    \"22\": {{\n",
    "        \"Title\": \"ì°¸ê³ ë¬¸í—Œ ì œëª©2\",\n",
    "        \"Counter\": 2,\n",
    "        \"Context\": [\"ì²« ë²ˆì§¸ ì¸ìš© ë¬¸ì¥\", \"ë‘ ë²ˆì§¸ ì¸ìš© ë¬¸ì¥\"]\n",
    "    }}\n",
    "}}\n",
    "```\n",
    "\n",
    "## References\n",
    "{references}\n",
    "\n",
    "## Essay\n",
    "{essay}\n",
    "\"\"\",\n",
    "\n",
    "    \"4\": \"\"\"\n",
    "ë…¼ë¬¸ ë³¸ë¬¸ì—ì„œ ì°¸ê³ ë¬¸í—Œì´ ì¸ìš©ëœ ê²½ìš°, í•´ë‹¹ ì°¸ê³ ë¬¸í—Œì˜ **ì¸ìš© íšŸìˆ˜**ì™€ **ì¸ìš© ë¬¸ì¥**ì„ ì •ë¦¬í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.  \n",
    "**ì¸ìš©ë˜ì§€ ì•Šì€ ì°¸ê³ ë¬¸í—Œì€ ì œì™¸í•©ë‹ˆë‹¤.**\n",
    "\n",
    "## ì¡°ê±´\n",
    "- ì¸ìš©í‘œì‹œ: ë…¼ë¬¸ ë³¸ë¬¸ì—ì„œ ì°¸ê³ ë¬¸í—Œì´ **[ìˆ«ì]** (`[4]`, `[18]` ë“±) í˜•ì‹ìœ¼ë¡œ ì¸ìš©ë˜ì—ˆì„ ê²½ìš°ë§Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n",
    "- ë™ì¼í•œ ì°¸ê³ ë¬¸í—Œì´ ì—¬ëŸ¬ ë²ˆ ì¸ìš©ëœ ê²½ìš° `\"Context\"` ë°°ì—´ì— í•´ë‹¹ ë¬¸ì¥ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "- `\"Context\"`ì˜ ê°œë³„ ë¬¸ì¥ì€ **300ì ì´í•˜**ë¡œ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì¶œë ¥ ì˜ˆì‹œ\n",
    "```json\n",
    "{{\n",
    "    \"4\": {{\n",
    "        \"Title\": \"ì°¸ê³ ë¬¸í—Œ ì œëª©1\",\n",
    "        \"Counter\": 5,\n",
    "        \"Context\": [\"ì²« ë²ˆì§¸ ì¸ìš© ë¬¸ì¥\", \"ë‘ ë²ˆì§¸ ì¸ìš© ë¬¸ì¥\", \"ì„¸ ë²ˆì§¸ ì¸ìš© ë¬¸ì¥\", \"ë„¤ ë²ˆì§¸ ì¸ìš© ë¬¸ì¥\", \"ë‹¤ì„¯ ë²ˆì§¸ ì¸ìš© ë¬¸ì¥\"]\n",
    "    }},\n",
    "    \"18\": {{\n",
    "        \"Title\": \"ì°¸ê³ ë¬¸í—Œ ì œëª©2\",\n",
    "        \"Counter\": 2,\n",
    "        \"Context\": [\"ì²« ë²ˆì§¸ ì¸ìš© ë¬¸ì¥\", \"ë‘ ë²ˆì§¸ ì¸ìš© ë¬¸ì¥\"]\n",
    "    }}\n",
    "}}\n",
    "```\n",
    "\n",
    "## References\n",
    "{references}\n",
    "\n",
    "## Essay\n",
    "{essay}\n",
    "\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¸ìš©íšŸìˆ˜ë¥¼ ì¹´ìš´íŒ…í•˜ì—¬ íƒ€ê²Ÿ ë…¼ë¬¸ê³¼ ì°¸ê³  ë…¼ë¬¸ ì‚¬ì´ì˜ í˜•ì‹ì  ê´€ë ¨ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤. \n",
    "n = 5\n",
    "result_ = []\n",
    "for index in range(n):\n",
    "    result = []\n",
    "    for key in reference_count_keys:\n",
    "        for essay in output[key]:\n",
    "            user_message = user_message_template_dict[str(index)].format(references=reference_dict, essay=essay)\n",
    "            response = message_to_openai(user_message)\n",
    "            try:\n",
    "                text = response.choices[0].message.content\n",
    "                text = re.sub(\"```json\",\"\",text)\n",
    "                text = re.sub(\"```\",\"\",text)\n",
    "                text_data = json.loads(text)\n",
    "                result.append(text_data)\n",
    "            except: \n",
    "                text_data = None\n",
    "            # items['References'] = text_data\n",
    "    result_.append(result)\n",
    "\n",
    "    for data in result:\n",
    "        for key, value_dict in data.items():\n",
    "            output[\"References\"][key]['Counter'] += value_dict['Counter']\n",
    "            output[\"References\"][key]['Context'].extend(value_dict['Context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_950/372705134.py:13: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "2 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "3 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "4 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "5 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "6 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "7 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "8 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "9 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "10 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "11 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "12 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "13 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "14 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "15 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "16 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\n",
      "    Measuring causal effects of data statistics on language modelâ€™s â€™factualâ€™ predictions\n",
      "17 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "18 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "19 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "20 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "21 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\n",
      "    Olmo: Accelerating the science of language models\n",
      "22 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\n",
      "    Investigating learning dynamics of bert fine-tuning\n",
      "23 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\n",
      "    Training compute-optimal large language models\n",
      "24 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "25 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "26 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "27 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "28 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "29 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "30 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\n",
      "    Starcoder: may the source be with you!\n",
      "31 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "32 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "33 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "34 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "35 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "36 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "37 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "38 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "39 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "40 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "41 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "42 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\n",
      "    Noise-robust de-duplication at scale\n",
      "43 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "44 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "45 ë²ˆì§¸ ë…¼ë¬¸ ì˜ˆì™¸ ë°œìƒ:  Page of results was unexpectedly empty (https://export.arxiv.org/api/query?search_query=Emergent+structures+and+training+dynamics+in+large+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=9&max_results=100)\n",
      "45 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\n",
      "    Emergent structures and training dynamics in large language models\n",
      "46 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "47 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "48 ë²ˆì§¸ ë…¼ë¬¸ ì˜ˆì™¸ ë°œìƒ:  Page of results was unexpectedly empty (https://export.arxiv.org/api/query?search_query=Llama%3A+Open+and+efficient+foundation+language+models&id_list=&sortBy=relevance&sortOrder=descending&start=100&max_results=100)\n",
      "48 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "49 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "50 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "51 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "52 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n",
      "53 ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "def fetch_arxiv_paper(title, max_results=30):\n",
    "    \n",
    "    search = arxiv.Search(\n",
    "        query=title,\n",
    "        max_results=max_results, \n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    for result in search.results():\n",
    "        if title[10:-10].lower().replace(\" \", \"\") in result.title.lower().replace(\" \", \"\"):\n",
    "            return ( {\n",
    "                \"title\": result.title,\n",
    "                \"abstract\": result.summary,\n",
    "                \"pdf_url\": result.pdf_url\n",
    "            })\n",
    "        \n",
    "    return None \n",
    "\n",
    "def download_arxiv_pdf(pdf_url, save_path):\n",
    "    response = requests.get(pdf_url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                file.write(chunk)\n",
    "    \n",
    "for index in range(len(output['References'])):\n",
    "    title = output['References'][str(index+1)]['Title']\n",
    "    try :\n",
    "        paper_info = fetch_arxiv_paper(title)\n",
    "        if paper_info is None:\n",
    "            paper_info = fetch_arxiv_paper(title, 150)\n",
    "    except Exception as e:\n",
    "        print(index+1,\"ë²ˆì§¸ ë…¼ë¬¸ ì˜ˆì™¸ ë°œìƒ: \", e)\n",
    "        try :\n",
    "            paper_info = fetch_arxiv_paper(title, None)\n",
    "        except:\n",
    "            paper_info = None\n",
    "\n",
    "    if paper_info is not None:\n",
    "        pdf_url = paper_info['pdf_url']\n",
    "        abstract = paper_info['abstract']\n",
    "        output['References'][str(index+1)]['abstract'] = abstract\n",
    "        output['References'][str(index+1)]['pdf_url'] = pdf_url\n",
    "        save_path = Path(\"references\") / (str(index+1)+ \"-\" + paper_info['title'][:15]+\".pdf\")\n",
    "        download_arxiv_pdf(pdf_url, save_path)\n",
    "        print(index+1,\"ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\")\n",
    "    \n",
    "    else:\n",
    "        pdf_url = None\n",
    "        abstract = None\n",
    "        output['References'][str(index+1)]['abstract'] = abstract\n",
    "        output['References'][str(index+1)]['pdf_url'] = pdf_url\n",
    "        print(index+1,\"ë²ˆì§¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\")\n",
    "        print(f\"    {output['References'][str(index+1)]['Title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ìš´ì´ ì•ˆë°›ì•„ì§„ ë…¼ë¬¸ì€ ì œì™¸í•©ë‹ˆë‹¤. \n",
    "# í•´ë‹¹ ì…€ ìœ„ì— ìˆ˜ë™ìœ¼ë¡œ ë„£ìœ¼ë©´ ì´ë¥¼ ë°˜ì˜ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "filtered_reference_dict = { key: value for key, value in output['References'].items() if value['pdf_url'] is not None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ëœ ë…¼ë¬¸ì€ ì§ì ‘ ê°€ì ¸ì˜¤ê³ , abstractë„ ì§ì ‘ ì‘ì„±í•´ì„œ ë„£ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "# output['References']['16']['abstract'] = \"Large amounts of training data are one of the major reasons for the high performance of state-of-the-art NLP models. But what exactly in the training data causes a model to make a certain prediction? We seek to answer this question by providing a language for describing how training data influences predictions, through a causal framework. Importantly, our framework bypasses the need to retrain expensive models and allows us to estimate causal effects based on observational data alone. Addressing the problem of extracting factual knowledge from pretrained language models (PLMs), we focus on simple data statistics such as co-occurrence counts and show that these statistics do influence the predictions of PLMs, suggesting that such models rely on shallow heuristics. Our causal framework and our results demonstrate the importance of studying datasets and the benefits of causality for understanding NLP models.\"\n",
    "# output['References']['21']['abstract'] = \"Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, we have built OLMo, a competitive, truly Open Language Model, to enable the scientific study of language models. Unlike most prior efforts that have only released model weights and inference code, we release OLMo alongside open training data and training and evaluation code. We hope this release will empower the open research community and inspire a new wave of innovation.\"\n",
    "# output['References']['22']['abstract'] = \"\"\"\n",
    "# The recently introduced pre-trained language\n",
    "# model BERT advances the state-of-the-art on\n",
    "# many NLP tasks through the fine-tuning approach, but few studies investigate how the\n",
    "# fine-tuning process improves the model performance on downstream tasks. In this paper, we inspect the learning dynamics of BERT\n",
    "# fine-tuning with two indicators. We use JS\n",
    "# divergence to detect the change of the attention mode and use SVCCA distance to examine the change to the feature extraction mode\n",
    "# during BERT fine-tuning. We conclude that\n",
    "# BERT fine-tuning mainly changes the attention mode of the last layers and modifies the\n",
    "# feature extraction mode of the intermediate\n",
    "# and last layers. Moreover, we analyze the consistency of BERT fine-tuning between different random seeds and different datasets. In\n",
    "# summary, we provide a distinctive understanding of the learning dynamics of BERT finetuning, which sheds some light on improving\n",
    "# the fine-tuning results.\"\"\"\n",
    "# output['References']['23']['abstract'] = \"We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4Ã— more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.\"\n",
    "# output['References']['30']['abstract'] = \"The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.\"\n",
    "# output['References']['44']['abstract'] = \"State-of-the-art pre-trained language models have been shown to memorise facts and perform well with limited amounts of training data. To gain a better understanding of how these models learn, we study their generalisation and memorisation capabilities in noisy and low-resource scenarios. We find that the training of these models is almost unaffected by label noise and that it is possible to reach near-optimal results even on extremely noisy datasets. However, our experiments also show that they mainly learn from high-frequency patterns and largely fail when tested on low-resource tasks such as few-shot learning and rare entity recognition. To mitigate such limitations, we propose an extension based on prototypical networks that improves performance in low-resource named entity recognition tasks.\"\n",
    "# output['References']['45']['abstract'] = \"\"\"\n",
    "# Large language models have achieved success\n",
    "# on a number of downstream tasks, particularly\n",
    "# in a few and zero-shot manner. As a consequence, researchers have been investigating\n",
    "# both the kind of information these networks\n",
    "# learn and how such information can be encoded\n",
    "# in the parameters of the model. We survey\n",
    "# the literature on changes in the network during\n",
    "# training, drawing from work outside of NLP\n",
    "# when necessary, and on learned representations\n",
    "# of linguistic features in large language models.\n",
    "# We note in particular the lack of sufficient research on the emergence of functional units â€“\n",
    "# subsections of the network where related functions are grouped or organized â€“ within large\n",
    "# language models, and motivate future work that\n",
    "# grounds the study of language models in an\n",
    "# analysis of their changing internal structure during training time.\n",
    "# \"\"\"\n",
    "# output['References']['48']['abstract'] = \"\"\"\n",
    "# We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research\n",
    "# \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message_template = \"\"\"\n",
    "ë‹¤ìŒ ë¬¸ì¥ë“¤ì„ ë¶„ì„í•˜ì—¬ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë¬¸ì¥ì€ ëª¨ë‘ ì œê±°í•˜ê³ , ì„œë¡œ ë…ë¦½ì ì¸ ë¬¸ì¥ë§Œ ë‚¨ê²¨ì£¼ì„¸ìš”.\n",
    "\n",
    "### ì…ë ¥ ë¬¸ì¥ ëª©ë¡:\n",
    "{text_list}\n",
    "\n",
    "### ì¶œë ¥ í˜•ì‹:\n",
    "- ì¤‘ë³µë˜ê±°ë‚˜ ìœ ì‚¬í•œ ë¬¸ì¥ì„ ì œê±°í•œ í›„, ì˜ë¯¸ì ìœ¼ë¡œ ë…ë¦½ì ì¸ ë¬¸ì¥ë§Œ ë²ˆí˜¸ì™€ í•¨ê»˜ ë‚¨ê²¨ì£¼ì„¸ìš”.\n",
    "- ì¶œë ¥ ê²°ê³¼ëŠ” ì›ë˜ ë¬¸ì¥ì˜ ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì¤‘ë³µì„ í”¼í•˜ë„ë¡ ì •ì œí•´ì£¼ì„¸ìš”.\n",
    "\n",
    "### ê²°ê³¼ ì˜ˆì‹œ :\n",
    "1. [27] reported that the performance of LLMs adheres to a scaling law, correlating positively with both the model size and the size of the pretraining corpus.\n",
    "2. In Eq.1, the definition of the local acquisition maxima is also dependent on the injected knowledge k and the window size tw, but we write tLAM(q, i) for brevity.\n",
    "3. While our finding that effectivity remains unchanged for different stages of pretraining may seem contradictory to the widely known observation that the amount of pretraining data is a critical factor in the performance of LLMs [23, 27], we suggest a plausible hypothesis based on further observations in Â§4.3.\n",
    "\n",
    "ìœ„ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ìœ ì‚¬í•œ ë¬¸ì¥ì„ ì œê±°í•˜ê³  ì •ì œëœ ê²°ê³¼ë¥¼ ë°˜í™˜í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "\"\"\"\n",
    "ratio = 0.2\n",
    "nums = int(round(len(output['References']) * ratio, 0))\n",
    "related_reference = dict(sorted(filtered_reference_dict.items(), key=lambda x:x[1]['Counter'], reverse=True)[:nums])\n",
    "\n",
    "for index in related_reference.keys():\n",
    "    query_list = related_reference[index]['Context']\n",
    "    user_message = user_message_template.format(text_list=query_list)\n",
    "    response = message_to_openai(user_message)\n",
    "    related_reference[index]['Questions'] = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì¸ìš© ë…¼ë¬¸ê³¼ì˜ ê´€ë ¨ ì§€ì  ì •ë¦¬...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [04:45<00:00, 25.92s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "user_message_template = \"\"\"\n",
    "ë‹¤ìŒ ë¬¸ì„œë¥¼ ë³´ê³  ì§ˆë¬¸ì€ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì—¬ ì¶œë ¥í•´ì£¼ì„¸ìš”.\n",
    "ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ë° ê´€ë ¨ ì§€ì‹ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ê³ , ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ë¶€ë¶„ì„ ë…¼ë¬¸ì—ì„œ ê·¸ëŒ€ë¡œ ë°œì·Œí•´ ì£¼ì„¸ìš”.\n",
    "ê·¸ë¦¬ê³  ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ 500ì ë‚´ì™¸ì˜ í•œ ë‹¨ë½ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.\n",
    "### ì¸ìš© ë…¼ë¬¸ ì œëª© (Title): {title}\n",
    "### ì§ˆë¬¸ ëª©ë¡ (Questions): {questions}\n",
    "### ë¬¸ì„œ (Document): {essay}\n",
    "### ë‹µë³€ ì˜ˆì‹œ (Example Answer):\n",
    "ì¸ìš© ë…¼ë¬¸ ì œëª© (Title): <<ë…¼ë¬¸ ì œëª©>>\n",
    "1. **ì§ˆë¬¸ :** <<ì§ˆë¬¸1-í•œêµ­ì–´ë¡œ ë²ˆì—­ëœ ì§ˆë¬¸>>\n",
    "   - **ë‹µë³€ :** <<ì§ˆë¬¸1ì— ëŒ€í•œ í•œêµ­ì–´ ë‹µë³€ì´ë‚˜ ê´€ë ¨ ì§€ì‹ í•œ ë‹¨ë½(300ì ë‚´ì™¸)>>\n",
    "   - **ê·¼ê±° :** <<ì¸ìš© ë…¼ë¬¸ì—ì„œ í•´ë‹¹ ë‹µë³€ì„ ë’·ë°›ì¹¨í•˜ëŠ” ë¬¸ì¥>>\n",
    "\n",
    "2. **ì§ˆë¬¸ :** <<ì§ˆë¬¸2-í•œêµ­ì–´ë¡œ ë²ˆì—­ëœ ì§ˆë¬¸>>\n",
    "   - **ë‹µë³€ :** <<ì§ˆë¬¸2ì— ëŒ€í•œ í•œêµ­ì–´ ë‹µë³€ì´ë‚˜ ê´€ë ¨ ì§€ì‹ í•œ ë‹¨ë½(300ì ë‚´ì™¸)>>\n",
    "   - **ê·¼ê±° :** <<ì¸ìš© ë…¼ë¬¸ì—ì„œ í•´ë‹¹ ë‹µë³€ì„ ë’·ë°›ì¹¨í•˜ëŠ” ë¬¸ì¥>>\n",
    "...\n",
    "\n",
    "ë‹µë³€ ìš”ì•½ \n",
    ": <<ì „ì²´ ë‹µë³€ì— ëŒ€í•œ ìš”ì•½ í•œ ë‹¨ë½(500ì ë‚´ì™¸)>>\n",
    "### ë‹µë³€ ìš”ì•½ (Summary of Answers)\n",
    "\"\"\"\n",
    "\n",
    "user_message_template_2 = \"\"\"\n",
    "ë‹¤ìŒì€ ë…¼ë¬¸ê³¼ í•´ë‹¹ ë…¼ë¬¸ì˜ ì°¸ê³ ë¬¸í—Œì— ê´€í•œ ë‚´ìš©ì…ë‹ˆë‹¤.\n",
    "í•´ë‹¹ ë…¼ë¬¸ì´ ì¸ìš© ë…¼ë¬¸ì—ì„œì˜ ì£¼ìš” ì§ˆì˜ ì‘ë‹µì— ëŒ€í•˜ì—¬ ì–´ë–»ê²Œ ì—°êµ¬ë¥¼ ë°œì „ì‹œì¼°ëŠ”ì§€ í•œ ë‹¨ë½ìœ¼ë¡œ ì„œìˆ í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "### í•´ë‹¹ ë…¼ë¬¸ ì œëª© (Title): {title}\n",
    "### í•´ë‹¹ ë…¼ë¬¸ ë‚´ìš© (Essay): {essay}\n",
    "### ì°¸ê³  ë¬¸í—Œì— ëŒ€í•œ ì£¼ìš” ì§ˆì˜ ì‘ë‹µ(QnA): {qna}\n",
    "\n",
    "### ë‹µë³€ ì˜ˆì‹œ (Example Answer):\n",
    "ì¸ìš© ë…¼ë¬¸ ì œëª© (Title): <<ë…¼ë¬¸ ì œëª©>>\n",
    "<<ì°¸ê³  ë¬¸í—Œì— ëŒ€í•œ ì£¼ìš” ì§ˆì˜ ì‘ë‹µì— ëŒ€í•˜ì—¬ ì–´ë–»ê²Œ ë…¼ë¬¸ì´ ì—°êµ¬ë¥¼ ë°œì „ì‹œì¼°ëŠ”ì§€ 300ì ë‚´ì™¸ë¡œ ì„œìˆ >>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "data_dir = Path('references')\n",
    "threshold = 25\n",
    "main_essay = \"\\n\\n\".join([output[key] for key in ['Title','Authors','Abstract']])\n",
    "for key in ['Introduction', 'Related Work', 'Experimental Setup', 'Results', 'Discussion and Conclusions']:\n",
    "    for doc in output[key]:\n",
    "        main_essay += (doc.page_content + \"\\n\\n\")\n",
    "\n",
    "for index in tqdm(related_reference.keys(), desc=\"ì¸ìš© ë…¼ë¬¸ê³¼ì˜ ê´€ë ¨ ì§€ì  ì •ë¦¬...\"):\n",
    "   title = related_reference[index]['Title']\n",
    "   questions = related_reference[index]['Questions']\n",
    "   \n",
    "   # ë…¼ë¬¸ íŒŒì¼ ì°¾ê¸°\n",
    "   for path in data_dir.rglob(\"*.pdf\"):\n",
    "      if path.name.split(\"-\")[0] == index:\n",
    "         break\n",
    "   \n",
    "   # PDF ë¡œë“œ\n",
    "   loader = UnstructuredPDFLoader(path)\n",
    "   documents = loader.load()\n",
    "   essay = documents[0].page_content\n",
    "   essay = \"\\n\".join([text for text in essay.split(\"\\n\") if len(text) >= threshold])\n",
    "   user_message = user_message_template.format(essay = essay, questions=questions, title=title)\n",
    "   response = message_to_openai(user_message)\n",
    "   summary = response.choices[0].message.content\n",
    "   related_reference[index]['Summary'] = summary\n",
    "   \n",
    "   user_message_2 = user_message_template_2.format(title=output['Title'], essay=main_essay, qna = summary)\n",
    "   response = message_to_openai(user_message_2)\n",
    "   summary_qna = response.choices[0].message.content\n",
    "   related_reference[index]['Summary_QnA'] = summary_qna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¸ìš© ë…¼ë¬¸ ì œëª© (Title): Deduplicating training data makes language models better\n",
      "\n",
      "1. **ì§ˆë¬¸ :** LLMsëŠ” í›ˆë ¨ ë°ì´í„°ì˜ ìƒë‹¹ ë¶€ë¶„ì„ ê¸°ì–µí•œë‹¤. ëª¨ë¸ì˜ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ í›ˆë ¨ ë°ì´í„°ë¥¼ ê¸°ì–µí•˜ëŠ” ê²½í–¥ì´ ì¦ê°€í•˜ë©°, ì´ëŠ” ì§€ì‹ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì— í•´ë¥¼ ë¼ì¹˜ì§€ ì•ŠëŠ”ë‹¤.\n",
      "   - **ë‹µë³€ :** ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ í›ˆë ¨ ë°ì´í„°ì˜ ìƒë‹¹ ë¶€ë¶„ì„ ê¸°ì–µí•˜ëŠ” ê²½í–¥ì´ ìˆìœ¼ë©°, ëª¨ë¸ì˜ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ ì´ëŸ¬í•œ ê²½í–¥ì´ ë”ìš± ë‘ë“œëŸ¬ì§„ë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ê¸°ì–µì€ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ”ë‹¤. ì—°êµ¬ì— ë”°ë¥´ë©´, LLMì€ í›ˆë ¨ ë°ì´í„°ì˜ 1% ì´ìƒì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ëª¨ë¸ì˜ í¬ê¸°ì™€ ê´€ê³„ì—†ì´ ë°œìƒí•œë‹¤.\n",
      "   - **ê·¼ê±° :** \"Over 1% of tokens emitted unprompted from a model trained on standard datasets (e.g., C4) are part of a memorized sequence.\"\n",
      "\n",
      "2. **ì§ˆë¬¸ :** ìš°ë¦¬ëŠ” LLMì´ ë¹„ì¸ê¸° ì§€ì‹ì„ ìŠµë“í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªëŠ”ë‹¤ê³  ê°€ì •í•œë‹¤. ì´ëŠ” ì¶©ë¶„í•œ ë…¸ì¶œì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ìš°ë¦¬ì˜ ë°œê²¬ì€ ì‚¬ì „ í›ˆë ¨ ì½”í¼ìŠ¤ë¥¼ ì¤‘ë³µ ì œê±°í•˜ë©´ LLM ì„±ëŠ¥ì´ í–¥ìƒëœë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•œë‹¤.\n",
      "   - **ë‹µë³€ :** LLMì€ ë¹„ì¸ê¸° ì§€ì‹ì„ ìŠµë“í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìœ¼ë©°, ì´ëŠ” ì¶©ë¶„í•œ ë…¸ì¶œì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì¤‘ë³µ ì œê±°ë¥¼ í†µí•´ í›ˆë ¨ ë°ì´í„°ì˜ í’ˆì§ˆì„ ë†’ì´ë©´ LLMì˜ ì„±ëŠ¥ì´ í–¥ìƒëœë‹¤. ì¤‘ë³µëœ ì‹œí€€ìŠ¤ì— ë” ë†’ì€ í™•ë¥ ì„ ë¶€ì—¬í•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê³ , ìŠµë“í•œ ì¼ë°˜í™”ë¥¼ ë” ì˜¤ë˜ ìœ ì§€í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ”ë‹¤.\n",
      "   - **ê·¼ê±° :** \"Our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences.\"\n",
      "\n",
      "3. **ì§ˆë¬¸ :** ìš°ë¦¬ì˜ ë°œê²¬ì€ ì‚¬ì „ í›ˆë ¨ ì½”í¼ìŠ¤ë¥¼ ì¤‘ë³µ ì œê±°í•˜ë©´ LLM ì„±ëŠ¥ì´ í–¥ìƒëœë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•œë‹¤.\n",
      "   - **ë‹µë³€ :** ì‚¬ì „ í›ˆë ¨ ì½”í¼ìŠ¤ë¥¼ ì¤‘ë³µ ì œê±°í•˜ë©´ LLMì˜ ì„±ëŠ¥ì´ í–¥ìƒëœë‹¤ëŠ” ì—°êµ¬ ê²°ê³¼ê°€ ìˆë‹¤. ì¤‘ë³µ ì œê±°ëŠ” ëª¨ë¸ì´ ì¤‘ë³µëœ ì‹œí€€ìŠ¤ì— ë†’ì€ í™•ë¥ ì„ ë¶€ì—¬í•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê³ , ìŠµë“í•œ ì¼ë°˜í™”ë¥¼ ë” ì˜¤ë˜ ìœ ì§€í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ”ë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ì¶œë ¥ì—ì„œ ê¸°ì–µëœ í…ìŠ¤íŠ¸ì˜ ë¹ˆë„ë¥¼ ì¤„ì´ëŠ” ë° ê¸°ì—¬í•œë‹¤.\n",
      "   - **ê·¼ê±° :** \"Deduplication allows us to train models that emit memorized text ten times less frequently.\"\n",
      "\n",
      "ë‹µë³€ ìš”ì•½ \n",
      ": ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ í›ˆë ¨ ë°ì´í„°ì˜ ìƒë‹¹ ë¶€ë¶„ì„ ê¸°ì–µí•˜ëŠ” ê²½í–¥ì´ ìˆìœ¼ë©°, ëª¨ë¸ì˜ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ ì´ëŸ¬í•œ ê²½í–¥ì´ ë”ìš± ë‘ë“œëŸ¬ì§„ë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ê¸°ì–µì€ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì— í•´ë¥¼ ë¼ì¹˜ì§€ ì•ŠëŠ”ë‹¤. LLMì€ ë¹„ì¸ê¸° ì§€ì‹ì„ ìŠµë“í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìœ¼ë©°, ì´ëŠ” ì¶©ë¶„í•œ ë…¸ì¶œì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì¤‘ë³µ ì œê±°ë¥¼ í†µí•´ í›ˆë ¨ ë°ì´í„°ì˜ í’ˆì§ˆì„ ë†’ì´ë©´ LLMì˜ ì„±ëŠ¥ì´ í–¥ìƒëœë‹¤. ì¤‘ë³µëœ ì‹œí€€ìŠ¤ì— ë” ë†’ì€ í™•ë¥ ì„ ë¶€ì—¬í•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê³ , ìŠµë“í•œ ì¼ë°˜í™”ë¥¼ ë” ì˜¤ë˜ ìœ ì§€í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ”ë‹¤. ì´ëŸ¬í•œ ì—°êµ¬ ê²°ê³¼ëŠ” LLMì˜ í›ˆë ¨ ë°ì´í„° ì¤‘ë³µ ì œê±°ê°€ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•œë‹¤.\n",
      "\n",
      "ì¸ìš© ë…¼ë¬¸ ì œëª© (Title): Deduplicating training data makes language models better\n",
      "\n",
      "í•´ë‹¹ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì‚¬ì‹¤ ì§€ì‹ ìŠµë“ ë©”ì»¤ë‹ˆì¦˜ì„ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•˜ë©°, ì¤‘ë³µ ì œê±°ê°€ LLM ì„±ëŠ¥ í–¥ìƒì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê°•ì¡°í•œë‹¤. LLMì´ ë¹„ì¸ê¸° ì§€ì‹ì„ ìŠµë“í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªëŠ” ì´ìœ ë¥¼ ì¶©ë¶„í•œ ë…¸ì¶œì˜ í•„ìš”ì„±ìœ¼ë¡œ ì„¤ëª…í•˜ê³ , ì¤‘ë³µ ì œê±°ê°€ ëª¨ë¸ì´ ì¤‘ë³µëœ ì‹œí€€ìŠ¤ì— ë†’ì€ í™•ë¥ ì„ ë¶€ì—¬í•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ì—¬ ìŠµë“í•œ ì¼ë°˜í™”ë¥¼ ë” ì˜¤ë˜ ìœ ì§€í•˜ë„ë¡ ë•ëŠ”ë‹¤ëŠ” ì ì„ ì œì‹œí•œë‹¤. ì´ëŸ¬í•œ ë°œê²¬ì€ LLMì˜ í›ˆë ¨ ë°ì´í„° ì¤‘ë³µ ì œê±°ê°€ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤ëŠ” ê¸°ì¡´ ì—°êµ¬ë¥¼ ë’·ë°›ì¹¨í•˜ë©°, LLMì˜ ì‚¬ì‹¤ ì§€ì‹ ìŠµë“ ë™ì—­í•™ì— ëŒ€í•œ ì´í•´ë¥¼ ì‹¬í™”ì‹œí‚¨ë‹¤.\n",
      "####\n",
      "ì¸ìš© ë…¼ë¬¸ ì œëª© (Title): Memorization without overfitting: Analyzing the training dynamics of large language models\n",
      "\n",
      "1. **ì§ˆë¬¸ :** [46]ì€ ë‹¤ì–‘í•œ ì‚¬ì „ í›ˆë ¨ ì¡°ê±´ì—ì„œ LLMì˜ ì•”ê¸° ë° ë§ê° í–‰ë™ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.\n",
      "   - **ë‹µë³€ :** [46]ì˜ ì—°êµ¬ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì•”ê¸° ë° ë§ê° í–‰ë™ì„ ë‹¤ì–‘í•œ ì‚¬ì „ í›ˆë ¨ ì¡°ê±´ì—ì„œ ë¶„ì„í•˜ì˜€ìœ¼ë©°, ì´ ì—°êµ¬ëŠ” ëª¨ë¸ì˜ í¬ê¸°ì™€ í›ˆë ¨ ê³¼ì •ì—ì„œì˜ ì•”ê¸° ë™ì—­í•™ì„ ì´í•´í•˜ëŠ” ë° ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤. \n",
      "   - **ê·¼ê±° :** \"We empirically study exact memorization in causal and masked language modeling, across model sizes and throughout the training process.\"\n",
      "\n",
      "2. **ì§ˆë¬¸ :** í›ˆë ¨ ë‹¨ê³„ì™€ ìŠµë“í•œ ì‚¬ì‹¤ ì§€ì‹ì˜ ë§ê°ì€ ê±°ë“­ì œê³± ë²•ì¹™ ê´€ê³„ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "   - **ë‹µë³€ :** í›ˆë ¨ ë‹¨ê³„ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ëª¨ë¸ì´ ê¸°ì–µí•˜ëŠ” ë°ì´í„°ì˜ ë¹„ìœ¨ì´ ê°ì†Œí•˜ëŠ” ê²½í–¥ì´ ìˆìœ¼ë©°, ì´ëŠ” ê±°ë“­ì œê³± ë²•ì¹™ì— ë”°ë¼ ì„¤ëª…ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¦‰, í›ˆë ¨ ë‹¨ê³„ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ë§ê°ì´ ë” ë¹ ë¥´ê²Œ ì§„í–‰ë©ë‹ˆë‹¤.\n",
      "   - **ê·¼ê±° :** \"We find that larger language models memorize training data faster.\"\n",
      "\n",
      "3. **ì§ˆë¬¸ :** ë§ê°ì˜ ê¸°í•˜ê¸‰ìˆ˜ì  ê²½í–¥ì€ LLM í›ˆë ¨ì˜ ë‹¤ì–‘í•œ ì¸¡ë©´ì—ì„œ ë³´ê³ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "   - **ë‹µë³€ :** LLM í›ˆë ¨ì—ì„œ ë§ê°ì˜ ê¸°í•˜ê¸‰ìˆ˜ì  ê²½í–¥ì€ ì‚¬ì „ í›ˆë ¨ì—ì„œì˜ ì•”ê¸° ë° ì§€ì†ì ì¸ í•™ìŠµì—ì„œì˜ ì‘ì—… ì„±ëŠ¥ ë“± ì—¬ëŸ¬ ì¸¡ë©´ì—ì„œ ê´€ì°°ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ í›ˆë ¨ ì¤‘ì— ê¸°ì–µí•œ ì •ë³´ë¥¼ ìƒëŠ” ê²½í–¥ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
      "   - **ê·¼ê±° :** \"We show that forgetting curves have lower bounds â€” we coin this as the forgetting baseline.\"\n",
      "\n",
      "ë‹µë³€ ìš”ì•½ \n",
      ": ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ í›ˆë ¨ ë™ì—­í•™ì„ ë¶„ì„í•˜ë©°, íŠ¹íˆ ì•”ê¸°ì™€ ë§ê°ì˜ ê´€ê³„ë¥¼ íƒêµ¬í•©ë‹ˆë‹¤. [46]ì˜ ì—°êµ¬ëŠ” ë‹¤ì–‘í•œ ì‚¬ì „ í›ˆë ¨ ì¡°ê±´ì—ì„œ LLMì˜ ì•”ê¸° ë° ë§ê° í–‰ë™ì„ ë¶„ì„í•˜ì˜€ê³ , í›ˆë ¨ ë‹¨ê³„ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ëª¨ë¸ì´ ê¸°ì–µí•˜ëŠ” ë°ì´í„°ì˜ ë¹„ìœ¨ì´ ê°ì†Œí•˜ëŠ” ê²½í–¥ì´ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë˜í•œ, ë§ê°ì˜ ê¸°í•˜ê¸‰ìˆ˜ì  ê²½í–¥ì€ LLM í›ˆë ¨ì˜ ì—¬ëŸ¬ ì¸¡ë©´ì—ì„œ ê´€ì°°ë˜ë©°, ì´ëŠ” ëª¨ë¸ì´ í›ˆë ¨ ì¤‘ì— ê¸°ì–µí•œ ì •ë³´ë¥¼ ìƒëŠ” ê²½í–¥ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì—°êµ¬ ê²°ê³¼ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ í›ˆë ¨ ë° ì¼ë°˜í™” ëŠ¥ë ¥ì„ ì´í•´í•˜ëŠ” ë° ì¤‘ìš”í•œ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "ì¸ìš© ë…¼ë¬¸ ì œëª© (Title): Memorization without overfitting: Analyzing the training dynamics of large language models\n",
      "\n",
      "í•´ë‹¹ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ í›ˆë ¨ ë™ì—­í•™ì„ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•˜ë©°, íŠ¹íˆ ì•”ê¸°ì™€ ë§ê°ì˜ ê´€ê³„ë¥¼ íƒêµ¬í•©ë‹ˆë‹¤. [46]ì˜ ì—°êµ¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ, í›ˆë ¨ ë‹¨ê³„ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ëª¨ë¸ì´ ê¸°ì–µí•˜ëŠ” ë°ì´í„°ì˜ ë¹„ìœ¨ì´ ê°ì†Œí•˜ëŠ” ê²½í–¥ì„ í™•ì¸í•˜ì˜€ê³ , ì´ëŠ” ê±°ë“­ì œê³± ë²•ì¹™ì— ì˜í•´ ì„¤ëª…ë©ë‹ˆë‹¤. ë˜í•œ, ë§ê°ì˜ ê¸°í•˜ê¸‰ìˆ˜ì  ê²½í–¥ì´ LLM í›ˆë ¨ì˜ ì—¬ëŸ¬ ì¸¡ë©´ì—ì„œ ê´€ì°°ë˜ë©°, ì´ëŠ” ëª¨ë¸ì´ í›ˆë ¨ ì¤‘ì— ê¸°ì–µí•œ ì •ë³´ë¥¼ ìƒëŠ” ê²½í–¥ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ëŸ¬í•œ í†µì°°ì€ LLMì˜ ì‚¬ì‹¤ ì§€ì‹ ìŠµë“ ë° ì¼ë°˜í™” ëŠ¥ë ¥ì„ ì´í•´í•˜ëŠ” ë° ê¸°ì—¬í•˜ë©°, í›ˆë ¨ ì¡°ê±´ì˜ ìµœì í™”ë¥¼ ìœ„í•œ ê¸°ì´ˆ ìë£Œë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
      "####\n",
      "ì¸ìš© ë…¼ë¬¸ ì œëª© (Title): Physics of language models: Part 3.1, knowledge storage and extraction\n",
      "\n",
      "1. **ì§ˆë¬¸ :** ëª¨ë¸ì´ ì§€ì‹ì´ ì£¼ì…ëœ ë°°ì¹˜ë¡œ ì—…ë°ì´íŠ¸ëœ í›„, í”„ë¡œë¸Œì—ì„œ ì¸¡ì •ëœ ëª¨ë¸ì˜ ë¡œê·¸ í™•ë¥ ì€ ì¦‰ê°ì ì´ê³  ëšœë ·í•œ ì¦ê°€ë¥¼ ë³´ì¸ë‹¤. \n",
      "   - **ë‹µë³€ :** ëª¨ë¸ì´ ì§€ì‹ì´ ì£¼ì…ëœ ë°°ì¹˜ë¡œ ì—…ë°ì´íŠ¸ë˜ë©´, ë¡œê·¸ í™•ë¥ ì´ ì¦‰ê°ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ìƒˆë¡œìš´ ì§€ì‹ì„ íš¨ê³¼ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ì¶”ì¶œí•  ìˆ˜ ìˆìŒì„ ë‚˜íƒ€ë‚¸ë‹¤.\n",
      "   - **ê·¼ê±° :** \"the modelâ€™s log probability measured on the probes shows an immediate and distinctive increase, after the model is updated with the batch containing the injected knowledge.\"\n",
      "\n",
      "2. **ì§ˆë¬¸ :** [4]ëŠ” ì§€ì‹ì´ ì‹ ë¢°ì„± ìˆê²Œ ì¶”ì¶œë˜ê¸° ìœ„í•´ì„œëŠ” ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ì œê³µë˜ì–´ì•¼ í•œë‹¤ê³  ì…ì¦í–ˆë‹¤.\n",
      "   - **ë‹µë³€ :** ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ì œê³µëœ ì§€ì‹ì€ ëª¨ë¸ì´ ì´ë¥¼ ì‹ ë¢°ì„± ìˆê²Œ ì¶”ì¶œí•˜ëŠ” ë° í•„ìˆ˜ì ì´ë‹¤. ì´ëŠ” ì§€ì‹ì˜ ë‹¤ì–‘ì„±ì´ ëª¨ë¸ì˜ í•™ìŠµì— ê¸ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.\n",
      "   - **ê·¼ê±° :** \"knowledge should be presented in a diverse format during pretraining to be reliably extracted.\"\n",
      "\n",
      "3. **ì§ˆë¬¸ :** R(p, t) = 0ì€ tpreì—ì„œ ì£¼ì…ëœ ì§€ì‹ì„ í¬í•¨í•œ ë¯¸ë‹ˆë°°ì¹˜ë¡œ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•œ í›„ ì‚¬ì‹¤ì  ì§€ì‹ì˜ ë¡œê·¸ í™•ë¥  ê°œì„ ì´ ì™„ì „íˆ ì‚¬ë¼ì¡ŒìŒì„ ë‚˜íƒ€ë‚¸ë‹¤.\n",
      "   - **ë‹µë³€ :** R(p, t) = 0ì€ ì£¼ì…ëœ ì§€ì‹ì´ ëª¨ë¸ì— íš¨ê³¼ì ìœ¼ë¡œ ì €ì¥ë˜ì§€ ì•Šì•˜ìŒì„ ë‚˜íƒ€ë‚´ë©°, ì´ëŠ” ì§€ì‹ ì¶”ì¶œì˜ ì‹¤íŒ¨ë¥¼ ì˜ë¯¸í•œë‹¤.\n",
      "   - **ê·¼ê±° :** \"R(p, t) = 0 indicates that the improvement in the log probability of factual knowledge... is completely lost.\"\n",
      "\n",
      "4. **ì§ˆë¬¸ :** ìš°ë¦¬ëŠ” OLMoë¥¼ ì‹¤í—˜ì— ì‚¬ìš©í•œë‹¤. ì™œëƒí•˜ë©´ ëª¨ë¸ì˜ ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸, ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë° ë°°ì¹˜ ì‹œí€€ìŠ¤ ë°ì´í„°ê°€ ê³µê°œì ìœ¼ë¡œ ì œê³µë˜ê¸° ë•Œë¬¸ì´ë‹¤.\n",
      "   - **ë‹µë³€ :** OLMoëŠ” ê³µê°œëœ ë°ì´í„°ì™€ ì²´í¬í¬ì¸íŠ¸ ë•ë¶„ì— ì‹¤í—˜ì— ì í•©í•œ ëª¨ë¸ë¡œ ì„ íƒë˜ì—ˆë‹¤.\n",
      "   - **ê·¼ê±° :** \"we use OLMo for the experiments since the intermediate checkpoints, optimizer states, and batch sequence data for pretraining the model are made publicly available.\"\n",
      "\n",
      "5. **ì§ˆë¬¸ :** ìš°ë¦¬ê°€ ì¡°ì‚¬í•˜ëŠ” OLMo-7Bì˜ ëª¨ë“  ì‚¬ì „ í›ˆë ¨ ë‹¨ê³„ì—ì„œ ì´ëŸ¬í•œ íŒ¨í„´ì´ ì¼ê´€ë˜ê²Œ ë‚˜íƒ€ë‚œë‹¤.\n",
      "   - **ë‹µë³€ :** OLMo-7Bì˜ ëª¨ë“  ì‚¬ì „ í›ˆë ¨ ë‹¨ê³„ì—ì„œ ì§€ì‹ ì €ì¥ ë° ì¶”ì¶œì˜ íŒ¨í„´ì´ ì¼ê´€ë˜ê²Œ ë‚˜íƒ€ë‚˜ë©°, ì´ëŠ” ëª¨ë¸ì˜ í•™ìŠµ ê³¼ì •ì—ì„œ ì¤‘ìš”í•œ í†µì°°ì„ ì œê³µí•œë‹¤.\n",
      "   - **ê·¼ê±° :** \"These patterns are consistent across all pretraining stages of OLMo-7B we investigate.\"\n",
      "\n",
      "6. **ì§ˆë¬¸ :** OLMo-1B ì´ˆê¸° ì²´í¬í¬ì¸íŠ¸ì˜ ë…íŠ¹í•œ í–‰ë™ì€ ëª¨ë¸ì´ ì‚¬ì‹¤ì  ì§€ì‹ì„ ì•ˆì •ì ìœ¼ë¡œ ìŠµë“í•˜ê¸° ìœ„í•´ íŠ¹ì • ìˆ˜ì˜ í† í°ì— ëŒ€í•œ ì‚¬ì „ í›ˆë ¨ì´ í•„ìš”í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•œë‹¤.\n",
      "   - **ë‹µë³€ :** OLMo-1Bì˜ ì´ˆê¸° ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” í–‰ë™ì€ ëª¨ë¸ì´ ì‚¬ì‹¤ì  ì§€ì‹ì„ ì•ˆì •ì ìœ¼ë¡œ ìŠµë“í•˜ê¸° ìœ„í•´ ì¶©ë¶„í•œ ì–‘ì˜ ë°ì´í„°ì— ë…¸ì¶œë˜ì–´ì•¼ í•¨ì„ ì‹œì‚¬í•œë‹¤.\n",
      "   - **ê·¼ê±° :** \"the distinctive behavior of the OLMo-1B early checkpoint suggests that pretraining on a certain number of tokens may be required for the model to acquire factual knowledge stably.\"\n",
      "\n",
      "### ë‹µë³€ ìš”ì•½ (Summary of Answers)\n",
      "ì´ ë…¼ë¬¸ì—ì„œëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì§€ì‹ì„ ì–´ë–»ê²Œ ì €ì¥í•˜ê³  ì¶”ì¶œí•˜ëŠ”ì§€ë¥¼ ë‹¤ë£¬ë‹¤. ëª¨ë¸ì´ ì§€ì‹ì´ ì£¼ì…ëœ ë°°ì¹˜ë¡œ ì—…ë°ì´íŠ¸ë˜ë©´ ë¡œê·¸ í™•ë¥ ì´ ì¦‰ê°ì ìœ¼ë¡œ ì¦ê°€í•˜ë©°, ì´ëŠ” ëª¨ë¸ì´ ìƒˆë¡œìš´ ì§€ì‹ì„ íš¨ê³¼ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ì¶”ì¶œí•  ìˆ˜ ìˆìŒì„ ë‚˜íƒ€ë‚¸ë‹¤. ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ì œê³µëœ ì§€ì‹ì€ ì‹ ë¢°ì„± ìˆëŠ” ì¶”ì¶œì„ ìœ„í•´ í•„ìˆ˜ì ì´ë©°, ì£¼ì…ëœ ì§€ì‹ì´ íš¨ê³¼ì ìœ¼ë¡œ ì €ì¥ë˜ì§€ ì•Šìœ¼ë©´ ì¶”ì¶œì´ ì‹¤íŒ¨í•  ìˆ˜ ìˆë‹¤. OLMo ëª¨ë¸ì€ ê³µê°œëœ ë°ì´í„° ë•ë¶„ì— ì‹¤í—˜ì— ì í•©í•˜ë©°, OLMo-7Bì˜ ëª¨ë“  ì‚¬ì „ í›ˆë ¨ ë‹¨ê³„ì—ì„œ ì¼ê´€ëœ íŒ¨í„´ì´ ë‚˜íƒ€ë‚œë‹¤. OLMo-1Bì˜ ì´ˆê¸° ì²´í¬í¬ì¸íŠ¸ëŠ” ëª¨ë¸ì´ ì‚¬ì‹¤ì  ì§€ì‹ì„ ì•ˆì •ì ìœ¼ë¡œ ìŠµë“í•˜ê¸° ìœ„í•´ ì¶©ë¶„í•œ ì–‘ì˜ ë°ì´í„°ì— ë…¸ì¶œë˜ì–´ì•¼ í•¨ì„ ì‹œì‚¬í•œë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” LLMì˜ ì§€ì‹ ì €ì¥ ë° ì¶”ì¶œ ë©”ì»¤ë‹ˆì¦˜ì„ ì´í•´í•˜ëŠ” ë° ì¤‘ìš”í•œ í†µì°°ì„ ì œê³µí•œë‹¤.\n",
      "\n",
      "ì¸ìš© ë…¼ë¬¸ ì œëª© (Title): Physics of language models: Part 3.1, knowledge storage and extraction\n",
      "\n",
      "í•´ë‹¹ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì‚¬ì‹¤ì  ì§€ì‹ ìŠµë“ ë©”ì»¤ë‹ˆì¦˜ì„ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•˜ë©°, ì§€ì‹ì´ ì£¼ì…ëœ ë°°ì¹˜ë¡œ ì—…ë°ì´íŠ¸ëœ í›„ ëª¨ë¸ì˜ ë¡œê·¸ í™•ë¥ ì´ ì¦‰ê°ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” í˜„ìƒì„ ì„¤ëª…í•œë‹¤. ì´ëŠ” ëª¨ë¸ì´ ìƒˆë¡œìš´ ì§€ì‹ì„ íš¨ê³¼ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ì¶”ì¶œí•  ìˆ˜ ìˆìŒì„ ë‚˜íƒ€ë‚´ë©°, ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ì œê³µëœ ì§€ì‹ì´ ì‹ ë¢°ì„± ìˆëŠ” ì¶”ì¶œì— í•„ìˆ˜ì ì„ì„ ê°•ì¡°í•œë‹¤. ë˜í•œ, OLMo ëª¨ë¸ì˜ ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ì™€ ê³µê°œëœ ë°ì´í„°ì˜ í™œìš©ì€ ì‹¤í—˜ì˜ ì‹ ë¢°ì„±ì„ ë†’ì´ë©°, OLMo-1B ì´ˆê¸° ì²´í¬í¬ì¸íŠ¸ì˜ í–‰ë™ì€ ì•ˆì •ì ì¸ ì§€ì‹ ìŠµë“ì„ ìœ„í•œ ì¶©ë¶„í•œ ë°ì´í„° ë…¸ì¶œì˜ í•„ìš”ì„±ì„ ì‹œì‚¬í•œë‹¤. ì´ëŸ¬í•œ í†µì°°ì€ LLMì˜ ì§€ì‹ ì €ì¥ ë° ì¶”ì¶œ ë©”ì»¤ë‹ˆì¦˜ì— ëŒ€í•œ ì´í•´ë¥¼ ì‹¬í™”ì‹œí‚¤ê³ , í–¥í›„ ì—°êµ¬ ë°©í–¥ì„ ì œì‹œí•œë‹¤.\n",
      "####\n",
      "ì¸ìš© ë…¼ë¬¸ ì œëª© (Title): Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?\n",
      "\n",
      "1. **ì§ˆë¬¸ :** [44]ì™€ [46]ì€ ì–¸ì–´ ëª¨ë¸ ì‚¬ì „ í›ˆë ¨ì—ì„œ ê¸°ì–µí™”ì˜ ë™ì—­í•™ì— ì´ˆì ì„ ë§ì·„ë‹¤.\n",
      "   - **ë‹µë³€ :** ì´ ì—°êµ¬ëŠ” ì–¸ì–´ ëª¨ë¸ì˜ ì‚¬ì „ í›ˆë ¨ ê³¼ì •ì—ì„œ ê¸°ì–µí™”ì˜ ë™ì—­í•™ì„ ë¶„ì„í•˜ë©°, ëª¨ë¸ì´ ìƒˆë¡œìš´ ì§€ì‹ì„ í†µí•©í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªëŠ”ë‹¤ëŠ” ì ì„ ê°•ì¡°í•œë‹¤. íŠ¹íˆ, ìƒˆë¡œìš´ ì§€ì‹ì´ ê¸°ì¡´ ì§€ì‹ê³¼ ì¼ì¹˜í•˜ì§€ ì•Šì„ ê²½ìš°, ëª¨ë¸ì´ ì´ë¥¼ í•™ìŠµí•˜ëŠ” ì†ë„ê°€ ëŠë ¤ì§„ë‹¤ëŠ” ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤.\n",
      "   - **ê·¼ê±° :** \"We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the modelâ€™s knowledge.\"\n",
      "\n",
      "2. **ì§ˆë¬¸ :** ì—¬ëŸ¬ ì—°êµ¬ë“¤ì´ LLMì˜ í›ˆë ¨ ë™ì—­í•™ì„ ì¡°ì‚¬í–ˆìœ¼ë©°, íŠ¹íˆ í›ˆë ¨ ì¤‘ ì–´ë–»ê²Œ ë°œì „í•˜ëŠ”ì§€ë¥¼ ë‹¤ë£¨ì—ˆë‹¤ [12, 18, 22, 32, 33, 45, 51].\n",
      "   - **ë‹µë³€ :** LLMì˜ í›ˆë ¨ ë™ì—­í•™ì— ëŒ€í•œ ì—°êµ¬ëŠ” ëª¨ë¸ì´ í›ˆë ¨ ì¤‘ì— ì–´ë–»ê²Œ ì§€ì‹ì„ ìŠµë“í•˜ê³  í™œìš©í•˜ëŠ”ì§€ë¥¼ ë¶„ì„í•˜ë©°, íŠ¹íˆ ìƒˆë¡œìš´ ì§€ì‹ì˜ í†µí•©ì´ ê¸°ì¡´ ì§€ì‹ì˜ í™œìš©ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ íƒêµ¬í•œë‹¤.\n",
      "   - **ê·¼ê±° :** \"We study how learning new factual knowledge through fine-tuning impacts the modelâ€™s tendency to hallucinate w.r.t. its pre-existing knowledge.\"\n",
      "\n",
      "3. **ì§ˆë¬¸ :** ìš°ë¦¬ëŠ” LLMì´ ë¹„ì¸ê¸° ì§€ì‹ì„ ìŠµë“í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªëŠ”ë‹¤ê³  ê°€ì •í•œë‹¤. ì´ëŠ” ì¶©ë¶„í•œ ë…¸ì¶œì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì´ë‹¤.\n",
      "   - **ë‹µë³€ :** LLMì€ ë¹„ì¸ê¸° ì§€ì‹ì„ ìŠµë“í•˜ê¸° ìœ„í•´ì„œëŠ” ì‚¬ì‹¤ì  ì§€ì‹ì— ëŒ€í•œ ì¶©ë¶„í•œ ë…¸ì¶œì´ í•„ìš”í•˜ë©°, ì´ ë…¸ì¶œì´ í•™ìŠµ ê°€ëŠ¥ì„±ì˜ ì„ê³„ê°’ë³´ë‹¤ ì§§ì€ ê°„ê²©ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ì•¼ í•œë‹¤.\n",
      "   - **ê·¼ê±° :** \"We hypothesize that LLMs struggle to acquire unpopular knowledge because they need sufficient exposure to factual knowledge with intervals shorter than the learnability threshold to increase the probability.\"\n",
      "\n",
      "4. **ì§ˆë¬¸ :** ìµœê·¼ [18]ì€ ë°ì´í„° í¬ê¸°ì™€ grokking ê°„ì˜ ê´€ê³„ë¥¼ íƒêµ¬í–ˆë‹¤.\n",
      "   - **ë‹µë³€ :** ë°ì´í„° í¬ê¸°ì™€ grokking ê°„ì˜ ê´€ê³„ë¥¼ íƒêµ¬í•œ ì—°êµ¬ëŠ” LLMì´ í›ˆë ¨ ì¤‘ì— ì–¼ë§ˆë‚˜ íš¨ê³¼ì ìœ¼ë¡œ ì§€ì‹ì„ ìŠµë“í•˜ëŠ”ì§€ë¥¼ ë¶„ì„í•˜ë©°, ë°ì´í„°ì˜ ì–‘ì´ ëª¨ë¸ì˜ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í‰ê°€í•œë‹¤.\n",
      "   - **ê·¼ê±° :** \"We explore how LLMs acquire and retain factual knowledge in terms of memorization and generalization by examining the following factors.\"\n",
      "\n",
      "5. **ì§ˆë¬¸ :** ìš°ë¦¬ëŠ” LLMì˜ ë¡œê·¸ í™•ë¥  ê°œì„ ì„ ì •ëŸ‰í™”í•˜ê¸° ìœ„í•´ íš¨ê³¼ì„±ì„ ì¸¡ì •í•œë‹¤.\n",
      "   - **ë‹µë³€ :** LLMì˜ ë¡œê·¸ í™•ë¥  ê°œì„ ì„ ì •ëŸ‰í™”í•˜ê¸° ìœ„í•´ íš¨ê³¼ì„±ì„ ì¸¡ì •í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬, ìƒˆë¡œìš´ ì§€ì‹ì´ ëª¨ë¸ì˜ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•œë‹¤.\n",
      "   - **ê·¼ê±° :** \"Next, we measure effectivity (Eq. 2) to quantify the improvement of the LLMsâ€™ log probability after being trained with the injected knowledge.\"\n",
      "\n",
      "6. **ì§ˆë¬¸ :** ì´ëŠ” ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¡œ í›ˆë ¨ëœ ëª¨ë¸ì´ ì§§ì€ í•™ìŠµ ê°€ëŠ¥ì„± ì„ê³„ê°’ì„ ê°€ì§„ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.\n",
      "   - **ë‹µë³€ :** ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¡œ í›ˆë ¨ëœ ëª¨ë¸ì€ ì§§ì€ í•™ìŠµ ê°€ëŠ¥ì„± ì„ê³„ê°’ì„ ê°€ì§€ë©°, ì´ëŠ” ëª¨ë¸ì´ ìƒˆë¡œìš´ ì§€ì‹ì„ íš¨ê³¼ì ìœ¼ë¡œ ìŠµë“í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªëŠ”ë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ë‚¸ë‹¤.\n",
      "   - **ê·¼ê±° :** \"This implies that the models trained with smaller batch sizes have shorter learnability threshold, the point such that an LLM cannot learn the knowledge presented with intervals longer than that threshold.\"\n",
      "\n",
      "7. **ì§ˆë¬¸ :** ê·¸ëŸ¬ë‚˜ ë” í° ëª¨ë¸ì˜ ê²½ìš°, ì§€ì‹ ê´€ì°° í›„ ë¡œê·¸ í™•ë¥ ì˜ ì¦‰ê°ì ì¸ ê°œì„ ëŸ‰ì€ í¬ê²Œ ì¦ê°€í•˜ì§€ ì•ŠëŠ”ë‹¤.\n",
      "   - **ë‹µë³€ :** ë” í° ëª¨ë¸ì˜ ê²½ìš°, ì§€ì‹ ê´€ì°° í›„ ë¡œê·¸ í™•ë¥ ì˜ ì¦‰ê°ì ì¸ ê°œì„ ëŸ‰ì€ ì¦ê°€í•˜ì§€ë§Œ, ì‚¬ì „ í›ˆë ¨ ì§„í–‰ ì¤‘ì—ëŠ” ê·¸ ì–‘ì´ í¬ê²Œ ì¦ê°€í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì´ ê´€ì°°ëœë‹¤.\n",
      "   - **ê·¼ê±° :** \"However, while the amount of immediate improvement in log probability upon observation of the knowledge increases for larger models, the amount does not significantly increase throughout the progress of pretraining.\"\n",
      "\n",
      "ë‹µë³€ ìš”ì•½ \n",
      ": ì´ ì—°êµ¬ëŠ” LLMì´ ìƒˆë¡œìš´ ì‚¬ì‹¤ì  ì§€ì‹ì„ í†µí•©í•˜ëŠ” ê³¼ì •ì—ì„œ ê²ªëŠ” ì–´ë ¤ì›€ê³¼ ê·¸ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” í™˜ê° í˜„ìƒì— ëŒ€í•´ ë¶„ì„í•œë‹¤. LLMì€ ì‚¬ì „ í›ˆë ¨ì„ í†µí•´ ê¸°ì¡´ ì§€ì‹ì„ ìŠµë“í•˜ì§€ë§Œ, ìƒˆë¡œìš´ ì§€ì‹ì„ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë°ëŠ” ì‹œê°„ì´ ê±¸ë¦¬ë©°, ì´ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ ì €í•˜ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆë‹¤. íŠ¹íˆ, ë¹„ì¸ê¸° ì§€ì‹ì˜ ìŠµë“ì€ ì¶©ë¶„í•œ ë…¸ì¶œì´ í•„ìš”í•˜ë©°, ì´ëŠ” í•™ìŠµ ê°€ëŠ¥ì„±ì˜ ì„ê³„ê°’ê³¼ ê´€ë ¨ì´ ìˆë‹¤. ì—°êµ¬ ê²°ê³¼ëŠ” LLMì˜ í›ˆë ¨ ë™ì—­í•™ì„ ì´í•´í•˜ëŠ” ë° ì¤‘ìš”í•œ í†µì°°ì„ ì œê³µí•˜ë©°, ìƒˆë¡œìš´ ì§€ì‹ì„ ë„ì…í•  ë•Œì˜ ìœ„í—˜ì„±ì„ ê°•ì¡°í•œë‹¤.\n",
      "\n",
      "ì¸ìš© ë…¼ë¬¸ ì œëª© (Title): Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?\n",
      "\n",
      "í•´ë‹¹ ë…¼ë¬¸ì€ LLMì´ ìƒˆë¡œìš´ ì‚¬ì‹¤ì  ì§€ì‹ì„ í†µí•©í•˜ëŠ” ê³¼ì •ì—ì„œì˜ ì–´ë ¤ì›€ê³¼ ê·¸ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” í™˜ê° í˜„ìƒì„ ë¶„ì„í•˜ë©°, LLMì˜ í›ˆë ¨ ë™ì—­í•™ì— ëŒ€í•œ ì´í•´ë¥¼ ì‹¬í™”ì‹œí‚¨ë‹¤. íŠ¹íˆ, LLMì´ ë¹„ì¸ê¸° ì§€ì‹ì„ ìŠµë“í•˜ê¸° ìœ„í•´ì„œëŠ” ì¶©ë¶„í•œ ë…¸ì¶œì´ í•„ìš”í•˜ë©°, ì´ëŠ” í•™ìŠµ ê°€ëŠ¥ì„±ì˜ ì„ê³„ê°’ê³¼ ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆìŒì„ ê°•ì¡°í•œë‹¤. ì´ëŸ¬í•œ í†µì°°ì€ LLMì˜ ì„±ëŠ¥ ì €í•˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•œ ì „ëµì„ ì œì‹œí•˜ë©°, ìƒˆë¡œìš´ ì§€ì‹ì„ ë„ì…í•  ë•Œì˜ ìœ„í—˜ì„±ì„ ê²½ê³ í•œë‹¤. ì´ ì—°êµ¬ëŠ” LLMì˜ ì§€ì‹ ìŠµë“ ë©”ì»¤ë‹ˆì¦˜ì„ ë³´ë‹¤ ëª…í™•íˆ ì´í•´í•˜ëŠ” ë° ê¸°ì—¬í•˜ë©°, í–¥í›„ ì—°êµ¬ ë°©í–¥ì„ ì œì‹œí•œë‹¤.\n",
      "####\n",
      "ì¸ìš© ë…¼ë¬¸ ì œëª© (Title): Scaling laws for neural language models\n",
      "\n",
      "1. **ì§ˆë¬¸ :** [27]ì€ LLMì˜ ì„±ëŠ¥ì´ ëª¨ë¸ í¬ê¸°ì™€ ì‚¬ì „ í›ˆë ¨ ì½”í¼ìŠ¤ì˜ í¬ê¸°ì™€ ê¸ì •ì ìœ¼ë¡œ ìƒê´€ê´€ê³„ê°€ ìˆëŠ” ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì„ ë”°ë¥¸ë‹¤ê³  ë³´ê³ í–ˆìŠµë‹ˆë‹¤.\n",
      "   - **ë‹µë³€ :** LLMì˜ ì„±ëŠ¥ì€ ëª¨ë¸ì˜ í¬ê¸°ì™€ ë°ì´í„°ì…‹ì˜ í¬ê¸°ì— ë”°ë¼ ì¦ê°€í•˜ë©°, ì´ëŠ” ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì— ì˜í•´ ì„¤ëª…ë©ë‹ˆë‹¤. ì¦‰, ëª¨ë¸ì˜ í¬ê¸°ì™€ ë°ì´í„°ì…‹ì˜ í¬ê¸°ê°€ ì¦ê°€í• ìˆ˜ë¡ ì„±ëŠ¥ì´ í–¥ìƒëœë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
      "   - **ê·¼ê±° :** \"Performance has a power-law relationship with each of the three scale factors N, D, C when not bottlenecked by the other two.\"\n",
      "\n",
      "2. **ì§ˆë¬¸ :** ìš°ë¦¬ëŠ” ìœˆë„ìš° í¬ê¸°ë¥¼ tw = 50ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. ë‹¤ìŒìœ¼ë¡œ, ëª¨ë¸ì´ ië²ˆì§¸ë¡œ ì§€ì‹ì„ ì œê³µë°›ì€ í›„ ì‚¬ì‹¤ì  ì§€ì‹ì˜ ë¡œê·¸ í™•ë¥ ì—ì„œ ì¦‰ê°ì ì¸ ê°œì„ ì„ ì •ëŸ‰í™”í•˜ëŠ” ë©”íŠ¸ë¦­ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
      "   - **ë‹µë³€ :** ìœˆë„ìš° í¬ê¸°ë¥¼ ì„¤ì •í•œ í›„, ëª¨ë¸ì´ íŠ¹ì • ì§€ì‹ì„ ì œê³µë°›ì€ í›„ì˜ ë¡œê·¸ í™•ë¥  ê°œì„ ì„ ì¸¡ì •í•˜ê¸° ìœ„í•œ ë©”íŠ¸ë¦­ì„ ì •ì˜í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
      "   - **ê·¼ê±° :** \"Next, we define a metric to quantify the immediate improvement in the modelâ€™s log probability of factual knowledge after it is presented with the knowledge for the i-th time.\"\n",
      "\n",
      "3. **ì§ˆë¬¸ :** ì •ì˜ëœ ë©”íŠ¸ë¦­ì˜ ì¸¡ì •ì€ ê·¸ë¦¼ 1ì— ì„¤ëª…ë˜ì–´ ìˆìŠµë‹ˆë‹¤. íš¨ê³¼ì„±ê³¼ ìœ ì§€ ê°€ëŠ¥ì„±ì˜ ì¸¡ì •ì„ ìœ„í•´, ìš°ë¦¬ëŠ” 1.5ì˜ ê³„ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ IQR ë°©ë²•ì„ ì´ìš©í•œ ì´ìƒì¹˜ íƒì§€ë¥¼ ì ìš©í•©ë‹ˆë‹¤.\n",
      "   - **ë‹µë³€ :** íš¨ê³¼ì„±ê³¼ ìœ ì§€ ê°€ëŠ¥ì„±ì„ ì¸¡ì •í•˜ê¸° ìœ„í•´ IQR ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì´ìƒì¹˜ë¥¼ íƒì§€í•˜ê³ , ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
      "   - **ê·¼ê±° :** \"For the measurement of effectivity and retainability, we apply outlier detection using the IQR method with a factor of 1.5.\"\n",
      "\n",
      "4. **ì§ˆë¬¸ :** ê²°ê³¼ëŠ” ì¤‘ë³µ(ìƒë‹¨), íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ(ì¤‘ì•™), í•œ ë²ˆ(í•˜ë‹¨) ì£¼ì… ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•´ ë³´ì—¬ì§‘ë‹ˆë‹¤.\n",
      "   - **ë‹µë³€ :** ë‹¤ì–‘í•œ ì£¼ì… ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•œ ê²°ê³¼ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.\n",
      "   - **ê·¼ê±° :** \"Results are shown for duplicate (Top), paraphrase (Center), and once (Bottom) injection scenarios.\"\n",
      "\n",
      "5. **ì§ˆë¬¸ :** íšë“ ê¹Šì´ì— ê´€ê³„ì—†ì´(ê¸°ì–µ, ì˜ë¯¸ ì¼ë°˜í™” ë° ì¡°í•© ì¼ë°˜í™”), ì£¼ì…ëœ ì§€ì‹ì„ í¬í•¨í•œ ë°°ì¹˜ë¡œ ëª¨ë¸ì´ ì—…ë°ì´íŠ¸ëœ í›„ í”„ë¡œë¸Œì—ì„œ ì¸¡ì •ëœ ëª¨ë¸ì˜ ë¡œê·¸ í™•ë¥ ì€ ì¦‰ê°ì ì´ê³  ëšœë ·í•œ ì¦ê°€ë¥¼ ë³´ì…ë‹ˆë‹¤.\n",
      "   - **ë‹µë³€ :** ëª¨ë¸ì´ ì£¼ì…ëœ ì§€ì‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸ëœ í›„, ë¡œê·¸ í™•ë¥ ì´ ì¦‰ê°ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” í˜„ìƒì„ ê´€ì°°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "   - **ê·¼ê±° :** \"the modelâ€™s log probability measured on the probes shows an immediate and distinctive increase, after the model is updated with the batch containing the injected knowledge.\"\n",
      "\n",
      "6. **ì§ˆë¬¸ :** ë‹¤ìŒìœ¼ë¡œ, ì£¼ì…ëœ ì§€ì‹ìœ¼ë¡œ í›ˆë ¨ëœ í›„ LLMì˜ ë¡œê·¸ í™•ë¥  ê°œì„ ì„ ì •ëŸ‰í™”í•˜ê¸° ìœ„í•´ íš¨ê³¼ì„±ì„ ì¸¡ì •í•©ë‹ˆë‹¤(Eq. 2).\n",
      "   - **ë‹µë³€ :** ì£¼ì…ëœ ì§€ì‹ìœ¼ë¡œ í›ˆë ¨ëœ í›„ LLMì˜ ë¡œê·¸ í™•ë¥  ê°œì„ ì„ ì •ëŸ‰í™”í•˜ê¸° ìœ„í•´ íš¨ê³¼ì„±ì„ ì¸¡ì •í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "   - **ê·¼ê±° :** \"Next, we measure effectivity (Eq. 2) to quantify the improvement of the LLMsâ€™ log probability after being trained with the injected knowledge.\"\n",
      "\n",
      "7. **ì§ˆë¬¸ :** ê·¸ë¦¼ 5ì˜ ì¶”ì •ëœ x-ì ˆí¸ì€ í›ˆë ¨ì„ í†µí•´ íšë“í•œ ì‚¬ì‹¤ì  ì§€ì‹ì˜ ì™„ì „í•œ ì†ì‹¤ë¡œ ì´ì–´ì§€ëŠ” ì¶”ê°€ í›ˆë ¨ í† í°ì˜ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
      "   - **ë‹µë³€ :** ê·¸ë¦¼ 5ì—ì„œ x-ì ˆí¸ì„ í†µí•´ ì¶”ê°€ í›ˆë ¨ í† í°ì˜ ìˆ˜ë¥¼ ì¶”ì •í•˜ì—¬ ì‚¬ì‹¤ì  ì§€ì‹ì˜ ì†ì‹¤ì„ ë¶„ì„í•©ë‹ˆë‹¤.\n",
      "   - **ê·¼ê±° :** \"The estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.\"\n",
      "\n",
      "8. **ì§ˆë¬¸ :** í›ˆë ¨ ë‹¨ê³„ì™€ íšë“í•œ ì‚¬ì‹¤ì  ì§€ì‹ì˜ ë§ê° ì‚¬ì´ì—ëŠ” í˜ ë²•ì¹™ ê´€ê³„ê°€ ìˆìŠµë‹ˆë‹¤.\n",
      "   - **ë‹µë³€ :** í›ˆë ¨ ë‹¨ê³„ì™€ ì‚¬ì‹¤ì  ì§€ì‹ì˜ ë§ê° ì‚¬ì´ì—ëŠ” í˜ ë²•ì¹™ ê´€ê³„ê°€ ì¡´ì¬í•˜ë©°, ì´ëŠ” ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.\n",
      "   - **ê·¼ê±° :** \"There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization.\"\n",
      "\n",
      "9. **ì§ˆë¬¸ :** ìš°ë¦¬ëŠ” LLMì´ ë¹„ì¸ê¸° ì§€ì‹ì„ ìŠµë“í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªëŠ”ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
      "   - **ë‹µë³€ :** LLMì€ ë¹„ì¸ê¸° ì§€ì‹ì„ ìŠµë“í•˜ëŠ” ë° í•„ìš”í•œ ì¶©ë¶„í•œ ë…¸ì¶œì´ ë¶€ì¡±í•˜ì—¬ ì–´ë ¤ì›€ì„ ê²ªëŠ”ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
      "   - **ê·¼ê±° :** \"We hypothesize that LLMs struggle to acquire unpopular knowledge because they need sufficient exposure to factual knowledge with intervals shorter than the learnability threshold to increase the probability.\"\n",
      "\n",
      "### ë‹µë³€ ìš”ì•½ (Summary of Answers)\n",
      "ì´ ë…¼ë¬¸ì—ì„œëŠ” LLMì˜ ì„±ëŠ¥ì´ ëª¨ë¸ í¬ê¸°ì™€ ë°ì´í„°ì…‹ í¬ê¸°ì— ë”°ë¼ ì¦ê°€í•˜ëŠ” ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì„ ë”°ë¥´ë©°, ì´ëŠ” ë‹¤ì–‘í•œ ì‹¤í—˜ì„ í†µí•´ ì…ì¦ë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸ì˜ ë¡œê·¸ í™•ë¥  ê°œì„ ì„ ì •ëŸ‰í™”í•˜ê¸° ìœ„í•œ ë©”íŠ¸ë¦­ì„ ì •ì˜í•˜ê³ , íš¨ê³¼ì„±ê³¼ ìœ ì§€ ê°€ëŠ¥ì„±ì„ ì¸¡ì •í•˜ê¸° ìœ„í•´ IQR ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì´ìƒì¹˜ë¥¼ íƒì§€í•©ë‹ˆë‹¤. ì£¼ì…ëœ ì§€ì‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸ëœ í›„ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¦‰ê°ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” í˜„ìƒì„ ê´€ì°°í•˜ì˜€ìœ¼ë©°, í›ˆë ¨ ë‹¨ê³„ì™€ ì‚¬ì‹¤ì  ì§€ì‹ì˜ ë§ê° ì‚¬ì´ì—ëŠ” í˜ ë²•ì¹™ ê´€ê³„ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ë˜í•œ, LLMì´ ë¹„ì¸ê¸° ì§€ì‹ì„ ìŠµë“í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªëŠ” ì´ìœ ë¥¼ ì„¤ëª…í•˜ë©°, ì´ëŸ¬í•œ ê²°ê³¼ë“¤ì€ LLMì˜ í›ˆë ¨ ë° ì„±ëŠ¥ ìµœì í™”ì— ëŒ€í•œ ì¤‘ìš”í•œ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "í•´ë‹¹ ë…¼ë¬¸ì€ LLMì˜ ì‚¬ì‹¤ì  ì§€ì‹ ìŠµë“ ë©”ì»¤ë‹ˆì¦˜ì„ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•˜ë©°, ì¸ìš© ë…¼ë¬¸ì—ì„œ ì œì‹œëœ ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ê³¼ ê´€ë ¨ëœ ì„±ëŠ¥ í–¥ìƒ ì›ì¸ì„ íƒêµ¬í•©ë‹ˆë‹¤. íŠ¹íˆ, ëª¨ë¸ í¬ê¸°ì™€ ë°ì´í„°ì…‹ í¬ê¸°ê°€ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì‹¤í—˜ì ìœ¼ë¡œ ê²€ì¦í•˜ê³ , ì£¼ì…ëœ ì§€ì‹ì— ëŒ€í•œ ì¦‰ê°ì ì¸ ë¡œê·¸ í™•ë¥  ê°œì„ ì„ ê´€ì°°í•¨ìœ¼ë¡œì¨ LLMì˜ í•™ìŠµ ë™ì—­í•™ì„ ëª…í™•íˆ í•©ë‹ˆë‹¤. ë˜í•œ, í›ˆë ¨ ë‹¨ê³„ì™€ ë§ê° ê°„ì˜ í˜ ë²•ì¹™ ê´€ê³„ë¥¼ ë°í˜€ë‚´ì–´ LLMì´ ë¹„ì¸ê¸° ì§€ì‹ì„ ìŠµë“í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªëŠ” ì´ìœ ë¥¼ ì„¤ëª…í•˜ë©°, ì´ëŸ¬í•œ í†µì°°ì€ LLMì˜ í›ˆë ¨ ë° ì„±ëŠ¥ ìµœì í™”ì— ì¤‘ìš”í•œ ê¸°ì—¬ë¥¼ í•©ë‹ˆë‹¤.\n",
      "####\n",
      "ì¸ìš© ë…¼ë¬¸ ì œëª© (Title): Language models are few-shot learners\n",
      "\n",
      "1. **ì§ˆë¬¸ :** ìµœê·¼ LLMì— ëŒ€í•œ ê´€ì‹¬ì´ ê¸‰ì¦í•˜ê³  ìˆë‹¤ [9, 13, 21, 23, 49].\n",
      "   - **ë‹µë³€ :** ìµœê·¼ ëª‡ ë…„ê°„ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì— ëŒ€í•œ ì—°êµ¬ì™€ ì‘ìš©ì´ ê¸‰ì¦í•˜ê³  ìˆìœ¼ë©°, ì´ëŠ” ìì—°ì–´ ì²˜ë¦¬(NLP) ë¶„ì•¼ì—ì„œì˜ í˜ì‹ ì„ ì´ëŒê³  ìˆë‹¤. LLMì€ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, íŠ¹íˆ ì ì€ ìˆ˜ì˜ ì˜ˆì‹œë¡œë„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ìœ¼ë¡œ ì£¼ëª©ë°›ê³  ìˆë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì€ ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¡œ ì‚¬ì „ í›ˆë ¨ë˜ì–´, ë‹¤ì–‘í•œ ì–¸ì–´ì  ê³¼ì œë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ê°–ì¶”ê³  ìˆë‹¤.\n",
      "   - **ê·¼ê±° :** \"Language models are few-shot learners\"ë¼ëŠ” ì œëª©ì—ì„œ ì–¸ê¸‰ëœ ë°”ì™€ ê°™ì´, LLMì€ ì ì€ ìˆ˜ì˜ ì˜ˆì‹œë¡œë„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ê°€ì§€ê³  ìˆë‹¤.\n",
      "\n",
      "2. **ì§ˆë¬¸ :** ìµœê·¼ì˜ ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ëŠ” ì² ì €íˆ ì¤‘ë³µ ì œê±°ê°€ ì´ë£¨ì–´ì¡Œë‹¤ [9, 28, 38, 43, 47, 48]. ë°ì´í„° ì¤‘ë³µ ì œê±°ê°€ ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ ë„ë¦¬ ê´€ì°°ë˜ê³  ìˆë‹¤ [1, 29, 42, 52].\n",
      "   - **ë‹µë³€ :** ìµœê·¼ì˜ ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ëŠ” ì¤‘ë³µ ì œê±° ê³¼ì •ì„ í†µí•´ ë°ì´í„°ì˜ ì§ˆì„ ë†’ì´ê³ , ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ê¸°ì—¬í•˜ê³  ìˆë‹¤. ì¤‘ë³µëœ ë°ì´í„°ê°€ ë§ì„ ê²½ìš°, ëª¨ë¸ì´ íŠ¹ì • íŒ¨í„´ì— ê³¼ì í•©ë  ìœ„í—˜ì´ ìˆìœ¼ë©°, ì´ëŠ” ì¼ë°˜í™” ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì¤‘ë³µ ì œê±°ëŠ” ëª¨ë¸ì˜ í•™ìŠµ íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” ì¤‘ìš”í•œ ê³¼ì •ìœ¼ë¡œ ì¸ì‹ë˜ê³  ìˆë‹¤.\n",
      "   - **ê·¼ê±° :** \"it is widely observed that data deduplication can improve model performance\"ë¼ëŠ” ë¬¸ì¥ì—ì„œ ë°ì´í„° ì¤‘ë³µ ì œê±°ê°€ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í•¨ì„ ëª…ì‹œí•˜ê³  ìˆë‹¤.\n",
      "\n",
      "3. **ì§ˆë¬¸ :** ë°ì´í„° ì¤‘ë³µ ì œê±°ê°€ ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ ë„ë¦¬ ê´€ì°°ë˜ê³  ìˆë‹¤ [1, 29, 42, 52].\n",
      "   - **ë‹µë³€ :** ë°ì´í„° ì¤‘ë³µ ì œê±°ëŠ” ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” ë° ìˆì–´ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ë©°, ì´ëŠ” ëª¨ë¸ì´ ë” ë‹¤ì–‘í•œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë•ëŠ”ë‹¤. ì¤‘ë³µëœ ë°ì´í„°ëŠ” ëª¨ë¸ì´ íŠ¹ì • ì˜ˆì‹œì— ê³¼ë„í•˜ê²Œ ì ì‘í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ì¼ë°˜í™” ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì¤‘ë³µ ì œê±°ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë†’ì´ëŠ” ë° í•„ìˆ˜ì ì´ë‹¤.\n",
      "   - **ê·¼ê±° :** \"data deduplication can improve model performance\"ë¼ëŠ” ë¬¸ì¥ì—ì„œ ë°ì´í„° ì¤‘ë³µ ì œê±°ì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ê³  ìˆë‹¤.\n",
      "\n",
      "### ë‹µë³€ ìš”ì•½ (Summary of Answers)\n",
      "ìµœê·¼ LLMì— ëŒ€í•œ ê´€ì‹¬ì´ ê¸‰ì¦í•˜ê³  ìˆìœ¼ë©°, ì´ëŠ” ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œì˜ í˜ì‹ ì„ ì´ëŒê³  ìˆë‹¤. LLMì€ ì ì€ ìˆ˜ì˜ ì˜ˆì‹œë¡œë„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ê°–ì¶”ê³  ìˆì–´ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤. ìµœê·¼ì˜ ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ëŠ” ì¤‘ë³µ ì œê±° ê³¼ì •ì„ í†µí•´ ë°ì´í„°ì˜ ì§ˆì„ ë†’ì´ê³ , ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ê¸°ì—¬í•˜ê³  ìˆë‹¤. ë°ì´í„° ì¤‘ë³µ ì œê±°ëŠ” ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” ë° ìˆì–´ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ë©°, ì´ëŠ” ëª¨ë¸ì´ ë” ë‹¤ì–‘í•œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë•ëŠ”ë‹¤. ì¤‘ë³µëœ ë°ì´í„°ëŠ” ëª¨ë¸ì´ íŠ¹ì • ì˜ˆì‹œì— ê³¼ë„í•˜ê²Œ ì ì‘í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ì¼ë°˜í™” ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì¤‘ë³µ ì œê±°ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë†’ì´ëŠ” ë° í•„ìˆ˜ì ì´ë‹¤.\n",
      "\n",
      "í•´ë‹¹ ë…¼ë¬¸ì€ LLMì˜ ì‚¬ì‹¤ì  ì§€ì‹ ìŠµë“ ë©”ì»¤ë‹ˆì¦˜ì„ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•¨ìœ¼ë¡œì¨, ê¸°ì¡´ ì—°êµ¬ì—ì„œ ì œê¸°ëœ ë°ì´í„° ì¤‘ë³µ ì œê±°ì˜ ì¤‘ìš”ì„±ê³¼ LLMì˜ ì„±ëŠ¥ í–¥ìƒ ê°„ì˜ ê´€ê³„ë¥¼ ë”ìš± ëª…í™•íˆ í–ˆë‹¤. íŠ¹íˆ, ì¤‘ë³µ ì œê±°ê°€ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì´ëŠ” ë° ê¸°ì—¬í•œë‹¤ëŠ” ì ì„ ê°•ì¡°í•˜ë©°, ì¤‘ë³µëœ ë°ì´í„°ê°€ ê³¼ì í•©ì„ ì´ˆë˜í•  ìˆ˜ ìˆìŒì„ ì§€ì í–ˆë‹¤. ë˜í•œ, LLMì˜ ì§€ì‹ ìŠµë“ ê³¼ì •ì—ì„œì˜ ë§¥ë½ì  ìš”ì¸ê³¼ í›ˆë ¨ ì¡°ê±´ì´ ì§€ì‹ì˜ ê¸°ì–µ ë° ë§ê°ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê·œëª…í•¨ìœ¼ë¡œì¨, LLMì˜ ì„±ëŠ¥ í–¥ìƒì— ëŒ€í•œ ìƒˆë¡œìš´ í†µì°°ì„ ì œê³µí•˜ì˜€ë‹¤. ì´ëŸ¬í•œ ì—°êµ¬ëŠ” LLMì˜ í›ˆë ¨ ë° í™œìš©ì— ìˆì–´ ë°ì´í„° í’ˆì§ˆì˜ ì¤‘ìš”ì„±ì„ ì¬í™•ì¸í•˜ëŠ” ë° ê¸°ì—¬í•œë‹¤.\n",
      "####\n",
      "ì¸ìš© ë…¼ë¬¸ ì œëª© (Title): Are emergent abilities of large language models a mirage?\n",
      "\n",
      "1. **ì§ˆë¬¸ :** LLMì˜ ì‚¬ì‹¤ì  ì§€ì‹ ìŠµë“ì„ ìƒì„¸íˆ ë¶„ì„í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ë¡œê·¸ í™•ë¥ ì„ ê²€í† í•˜ì—¬ ëª¨ë¸ì˜ ìƒíƒœë¥¼ í‰ê°€í•©ë‹ˆë‹¤. \n",
      "   - **ë‹µë³€ :** LLMì˜ ì‚¬ì‹¤ì  ì§€ì‹ ìŠµë“ì„ ë¶„ì„í•˜ê¸° ìœ„í•´ ë¡œê·¸ í™•ë¥ ì„ í†µí•´ ëª¨ë¸ì˜ ìƒíƒœë¥¼ í‰ê°€í•˜ëŠ” ë°©ë²•ì€ ëª¨ë¸ì´ ì§€ì‹ì„ ì–´ë–»ê²Œ ìŠµë“í•˜ëŠ”ì§€ë¥¼ ì„¸ë°€í•˜ê²Œ ì´í•´í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ë¡œê·¸ í™•ë¥ ì˜ ë³€í™”ë¥¼ ì¶”ì í•¨ìœ¼ë¡œì¨, ëª¨ë¸ì´ íŠ¹ì • ì§€ì‹ì„ ì–¼ë§ˆë‚˜ ì˜ ì¸ì‹í•˜ê³  ìˆëŠ”ì§€ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "   - **ê·¼ê±° :** \"To conduct a detailed analysis of the LLMsâ€™ acquisition of factual knowledge during pretraining, we evaluate the modelâ€™s state by examining log probabilities to obtain fine-grained information.\"\n",
      "\n",
      "2. **ì§ˆë¬¸ :** ì‚¬ì‹¤ì  ì§€ì‹ ìŠµë“ì˜ ì¶”ì„¸ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ë¨¼ì € ëª¨ë¸ ì—…ë°ì´íŠ¸ì˜ ì§€ì—­ì  íš¨ê³¼ê°€ ì™„ì „íˆ ë°˜ì˜ë˜ëŠ” ì‹œì ì„ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
      "   - **ë‹µë³€ :** ì‚¬ì‹¤ì  ì§€ì‹ ìŠµë“ì˜ ì¶”ì„¸ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •í•˜ê¸° ìœ„í•´ì„œëŠ” ëª¨ë¸ ì—…ë°ì´íŠ¸ê°€ íš¨ê³¼ë¥¼ ë°œíœ˜í•˜ê¸° ì‹œì‘í•˜ëŠ” ì‹œì ì„ ì •ì˜í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ ì‹œì ì€ ëª¨ë¸ì´ ìƒˆë¡œìš´ ì§€ì‹ì„ í†µí•©í•˜ê³  ì´ë¥¼ í™œìš©í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì´ ë°œíœ˜ë˜ëŠ” ìˆœê°„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
      "   - **ê·¼ê±° :** \"To quantitatively measure the trend of factual knowledge acquisition, we should first define the timestep where the local effect of updating the model using the injected knowledge completely pays off.\"\n",
      "\n",
      "3. **ì§ˆë¬¸ :** ëª¨ë¸ì€ ì‚¬ì „ í›ˆë ¨ì´ ì§„í–‰ë¨ì— ë”°ë¼ ì§€ì‹ì˜ ë¡œê·¸ í™•ë¥ ì´ ì¦ê°€í•˜ê³ , ì–´ëŠ ì‹œì ì—ì„œ ì´ ëˆ„ì ëœ ë¡œê·¸ í™•ë¥ ì´ ëª¨ë¸ì˜ ë””ì½”ë”© ì¶œë ¥ìœ¼ë¡œ ì§€ì‹ì„ ìƒì„±í•  ë§Œí¼ ì¶©ë¶„íˆ ë†’ì•„ì§ˆ ê²ƒì…ë‹ˆë‹¤.\n",
      "   - **ë‹µë³€ :** ëª¨ë¸ì€ ì‚¬ì „ í›ˆë ¨ ê³¼ì •ì—ì„œ ì§€ì‹ì— ëŒ€í•œ ë¡œê·¸ í™•ë¥ ì„ ëˆ„ì í•˜ì—¬, íŠ¹ì • ì‹œì ì— ì´ ëˆ„ì ëœ í™•ë¥ ì´ ì¶©ë¶„íˆ ë†’ì•„ì§€ë©´ í•´ë‹¹ ì§€ì‹ì„ ì¶œë ¥ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ì§€ì‹ì„ íš¨ê³¼ì ìœ¼ë¡œ ìŠµë“í•˜ê³  í™œìš©í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
      "   - **ê·¼ê±° :** \"The model will accumulate the increased log probability of the knowledge upon each encounter of the knowledge as the pretraining progresses, and at some point, the accumulated log probability of the knowledge will be high enough to generate the knowledge as the decoding output of the model.\"\n",
      "\n",
      "### ë‹µë³€ ìš”ì•½ (Summary of Answers)\n",
      "ì´ ë…¼ë¬¸ì—ì„œëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì‚¬ì‹¤ì  ì§€ì‹ ìŠµë“ ê³¼ì •ì„ ë¶„ì„í•˜ê¸° ìœ„í•´ ë¡œê·¸ í™•ë¥ ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ ì§€ì‹ì„ ì–´ë–»ê²Œ ìŠµë“í•˜ëŠ”ì§€ë¥¼ ì„¸ë°€í•˜ê²Œ ì´í•´í•  ìˆ˜ ìˆìœ¼ë©°, ëª¨ë¸ ì—…ë°ì´íŠ¸ì˜ íš¨ê³¼ê°€ ë‚˜íƒ€ë‚˜ëŠ” ì‹œì ì„ ì •ì˜í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤ê³  ê°•ì¡°í•©ë‹ˆë‹¤. ë˜í•œ, ëª¨ë¸ì€ ì‚¬ì „ í›ˆë ¨ ê³¼ì •ì—ì„œ ì§€ì‹ì— ëŒ€í•œ ë¡œê·¸ í™•ë¥ ì„ ëˆ„ì í•˜ì—¬, íŠ¹ì • ì‹œì ì— ì´ ëˆ„ì ëœ í™•ë¥ ì´ ì¶©ë¶„íˆ ë†’ì•„ì§€ë©´ í•´ë‹¹ ì§€ì‹ì„ ì¶œë ¥ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¶„ì„ì€ LLMì˜ ì§€ì‹ ìŠµë“ ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ë° ì¤‘ìš”í•œ ê¸°ì´ˆ ìë£Œë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "ì¸ìš© ë…¼ë¬¸ <<Are emergent abilities of large language models a mirage?>>ì—ì„œ ì œê¸°ëœ ì§ˆì˜ ì‘ë‹µì„ ë°”íƒ•ìœ¼ë¡œ, ë³¸ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì‚¬ì‹¤ì  ì§€ì‹ ìŠµë“ ê³¼ì •ì„ ë³´ë‹¤ ì„¸ë°€í•˜ê²Œ ë¶„ì„í•˜ëŠ” ë°©ë²•ë¡ ì„ ë°œì „ì‹œì¼°ìŠµë‹ˆë‹¤. íŠ¹íˆ, ë¡œê·¸ í™•ë¥ ì„ í†µí•´ ëª¨ë¸ì˜ ìƒíƒœë¥¼ í‰ê°€í•˜ê³ , ëª¨ë¸ ì—…ë°ì´íŠ¸ì˜ íš¨ê³¼ê°€ ë‚˜íƒ€ë‚˜ëŠ” ì‹œì ì„ ì •ì˜í•¨ìœ¼ë¡œì¨, LLMì´ ì§€ì‹ì„ ì–´ë–»ê²Œ ìŠµë“í•˜ê³  í™œìš©í•˜ëŠ”ì§€ë¥¼ ëª…í™•íˆ ë°í˜”ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì ‘ê·¼ì€ LLMì˜ ì§€ì‹ ìŠµë“ ëŠ¥ë ¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •í•  ìˆ˜ ìˆëŠ” ê¸°ì´ˆ ìë£Œë¥¼ ì œê³µí•˜ë©°, ëª¨ë¸ì´ ì‚¬ì „ í›ˆë ¨ ê³¼ì •ì—ì„œ ì§€ì‹ì˜ ë¡œê·¸ í™•ë¥ ì„ ëˆ„ì í•˜ì—¬ ì¶œë ¥ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆëŠ” ë©”ì»¤ë‹ˆì¦˜ì„ ì´í•´í•˜ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ LLMì˜ í•™ìŠµ ë™ì—­í•™ì— ëŒ€í•œ í†µì°°ì„ ì œê³µí•˜ê³ , í–¥í›„ ì—°êµ¬ ë°©í–¥ì„ ì œì‹œí•©ë‹ˆë‹¤.\n",
      "####\n",
      "ì¸ìš© ë…¼ë¬¸ ì œëª© (Title): To repeat or not to repeat: Insights from scaling llm under token-crisis\n",
      "\n",
      "1. **ì§ˆë¬¸ :** ê¸´ ê¼¬ë¦¬ ì§€ì‹ì„ íšë“í•˜ì§€ ëª»í•˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\n",
      "   - **ë‹µë³€ :** ê¸´ ê¼¬ë¦¬ ì§€ì‹ì˜ íšë“ ì‹¤íŒ¨ëŠ” LLMì´ ë¹„ì˜ì–´ ë°ì´í„°ì—ì„œ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” ê²ƒê³¼ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. LLMì€ ì˜ì–´ ë°ì´í„°ì— ë¹„í•´ ë¹„ì˜ì–´ ë°ì´í„°ì˜ ì–‘ì´ ì ê³ , ì´ë¡œ ì¸í•´ ë‹¤ì–‘í•œ ì–¸ì–´ì™€ ë¬¸í™”ì  ë§¥ë½ì„ ë°˜ì˜í•œ ì§€ì‹ì„ ì¶©ë¶„íˆ í•™ìŠµí•˜ì§€ ëª»í•©ë‹ˆë‹¤. ì´ë¡œ ì¸í•´ LLMì€ ë¹„ì˜ì–´ ì‘ì—…ì—ì„œ ì„±ëŠ¥ì´ ì €í•˜ë©ë‹ˆë‹¤.\n",
      "   - **ê·¼ê±° :** \"Despite PaLMâ€™s impressive 540B parameters and training on 780B tokens, including 22% non-English data, it still lags behind models such as mT5 and ByT5 on non-English tasks like Multilingual QA.\"\n",
      "\n",
      "2. **ì§ˆë¬¸ :** ì¤‘ë³µëœ í…ìŠ¤íŠ¸ë¥¼ ì œì‹œí•  ë•Œ ëª¨ë¸ì´ ì‚¬ì‹¤ì  ì§€ì‹ì„ ì¼ë°˜í™”í•˜ëŠ” ë° ë” ë¹¨ë¦¬ ìŠì–´ë²„ë¦¬ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\n",
      "   - **ë‹µë³€ :** ì¤‘ë³µëœ í…ìŠ¤íŠ¸ë¡œ í›ˆë ¨ëœ ëª¨ë¸ì€ ê³¼ì í•©ì— ë” ì·¨ì•½í•´ì§€ë©°, ì´ë¡œ ì¸í•´ ì¼ë°˜í™” ëŠ¥ë ¥ì´ ì €í•˜ë©ë‹ˆë‹¤. íŠ¹íˆ, ë°ì´í„°ê°€ ì œí•œì ì¼ ë•Œ ëª¨ë¸ì€ ë°˜ë³µëœ ì •ë³´ë¥¼ ë¹ ë¥´ê²Œ ê¸°ì–µí•˜ì§€ë§Œ, ì´ëŠ” ì‹¤ì œë¡œëŠ” ë” ë§ì€ ì •ë³´ë¥¼ ìŠê²Œ ë§Œë“œëŠ” ê²°ê³¼ë¥¼ ì´ˆë˜í•©ë‹ˆë‹¤.\n",
      "   - **ê·¼ê±° :** \"We observe that larger models are more susceptible to overfitting under token-crisis conditions.\"\n",
      "\n",
      "ë‹µë³€ ìš”ì•½ \n",
      ": ì´ ë…¼ë¬¸ì€ LLMì´ ê¸´ ê¼¬ë¦¬ ì§€ì‹ì„ íšë“í•˜ì§€ ëª»í•˜ëŠ” ì´ìœ ì™€ ì¤‘ë³µëœ í…ìŠ¤íŠ¸ë¡œ ì¸í•´ ì‚¬ì‹¤ì  ì§€ì‹ì„ ìŠì–´ë²„ë¦¬ëŠ” ê²½í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤. LLMì€ ë¹„ì˜ì–´ ë°ì´í„°ì˜ ë¶€ì¡±ìœ¼ë¡œ ì¸í•´ ë‹¤ì–‘í•œ ì–¸ì–´ì  ì§€ì‹ì„ ì¶©ë¶„íˆ í•™ìŠµí•˜ì§€ ëª»í•˜ë©°, ì´ëŠ” ë¹„ì˜ì–´ ì‘ì—…ì—ì„œ ì„±ëŠ¥ ì €í•˜ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤. ë˜í•œ, ì¤‘ë³µëœ ë°ì´í„°ë¡œ í›ˆë ¨ëœ ëª¨ë¸ì€ ê³¼ì í•©ì— ì·¨ì•½í•´ì ¸ ì¼ë°˜í™” ëŠ¥ë ¥ì´ ì €í•˜ë˜ë©°, ì´ëŠ” ëª¨ë¸ì´ ë°˜ë³µëœ ì •ë³´ë¥¼ ë¹ ë¥´ê²Œ ê¸°ì–µí•˜ì§€ë§Œ, ë™ì‹œì— ë” ë§ì€ ì •ë³´ë¥¼ ìŠê²Œ ë§Œë“œëŠ” ê²°ê³¼ë¥¼ ì´ˆë˜í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” LLMì˜ í›ˆë ¨ ë°©ì‹ê³¼ ë°ì´í„°ì˜ ì§ˆ, ì–‘ì— ë”°ë¼ í¬ê²Œ ì˜í–¥ì„ ë°›ìœ¼ë©°, í–¥í›„ LLMì˜ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ì„œëŠ” ë‹¤ì–‘í•œ ë°ì´í„°ì™€ í›ˆë ¨ ì „ëµì´ í•„ìš”í•¨ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì¸ìš© ë…¼ë¬¸ ì œëª© (Title): To repeat or not to repeat: Insights from scaling llm under token-crisis\n",
      "\n",
      "í•´ë‹¹ ë…¼ë¬¸ì€ LLMì´ ê¸´ ê¼¬ë¦¬ ì§€ì‹ì„ íšë“í•˜ì§€ ëª»í•˜ëŠ” ì´ìœ ì™€ ì¤‘ë³µëœ í…ìŠ¤íŠ¸ë¡œ ì¸í•´ ì‚¬ì‹¤ì  ì§€ì‹ì„ ìŠì–´ë²„ë¦¬ëŠ” ê²½í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì€ LLMì˜ í›ˆë ¨ ê³¼ì •ì—ì„œ ë¹„ì˜ì–´ ë°ì´í„°ì˜ ë¶€ì¡±ì´ ë‹¤ì–‘í•œ ì–¸ì–´ì  ì§€ì‹ì˜ í•™ìŠµì„ ì €í•´í•˜ë©°, ì¤‘ë³µëœ ë°ì´í„°ë¡œ í›ˆë ¨ëœ ëª¨ë¸ì´ ê³¼ì í•©ì— ì·¨ì•½í•´ì ¸ ì¼ë°˜í™” ëŠ¥ë ¥ì´ ì €í•˜ëœë‹¤ëŠ” ì ì„ ê°•ì¡°í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ í†µì°°ì€ LLMì˜ í›ˆë ¨ ë°©ì‹ê³¼ ë°ì´í„°ì˜ ì§ˆ, ì–‘ì´ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì´í•´í•˜ëŠ” ë° ê¸°ì—¬í•˜ë©°, í–¥í›„ LLMì˜ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ë‹¤ì–‘í•œ ë°ì´í„°ì™€ í›ˆë ¨ ì „ëµì˜ í•„ìš”ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.\n",
      "####\n",
      "ì¸ìš© ë…¼ë¬¸ ì œëª© (Title): Language models as knowledge bases?\n",
      "\n",
      "1. **ì§ˆë¬¸ :** ìµœê·¼ ì—°êµ¬ë“¤ì€ LLMì´ ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ì—ì„œ ìƒë‹¹í•œ ì‚¬ì‹¤ì  ì§€ì‹ì„ í¬ì°©í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì—ˆë‹¤ [14, 36, 40].\n",
      "   - **ë‹µë³€ :** ìµœê·¼ ì—°êµ¬ë“¤ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ì—ì„œ ìƒë‹¹í•œ ì‚¬ì‹¤ì  ì§€ì‹ì„ í¬ì°©í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤. LLMì€ ì •ë³´ ê²€ìƒ‰ ê¸°ìˆ ë³´ë‹¤ ì§€ì‹ ì§‘ì•½ì ì¸ ì‘ì—…ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ë©°, ìƒì„±ëœ ì§€ì‹ì˜ ì‚¬ì‹¤ì„±ì€ ë‹¤ì†Œ ë‚®ë”ë¼ë„ í•˜ìœ„ ì‘ì—…ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ í¬ì§€ ì•Šë‹¤ëŠ” ê²°ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤. \n",
      "   - **ê·¼ê±° :** \"LLM-generated knowledge surpasses retrieved knowledge in most evaluation perspectives, while it actually suffers from the factuality issue as expected.\"\n",
      "\n",
      "2. **ì§ˆë¬¸ :** LLMì˜ ë§¤ê°œë³€ìˆ˜ì— ì¸ì½”ë”©ëœ ì§€ì‹ì— ëŒ€í•œ ê´‘ë²”ìœ„í•œ ì—°êµ¬ê°€ ì§„í–‰ë˜ì—ˆë‹¤ [36, 40].\n",
      "   - **ë‹µë³€ :** LLMì˜ ë§¤ê°œë³€ìˆ˜ì— ì¸ì½”ë”©ëœ ì§€ì‹ì— ëŒ€í•œ ì—°êµ¬ëŠ” LLMì´ ìƒì„±í•˜ëŠ” ì§€ì‹ì˜ ì§ˆê³¼ ì‹ ë¢°ì„±ì„ í‰ê°€í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, LLMì´ ìƒì„±í•œ ì§€ì‹ì€ ì •ë³´ ê²€ìƒ‰ ëª¨ë¸ë³´ë‹¤ ë” ìœ ìš©í•˜ê³  ê´€ë ¨ì„±ì´ ë†’ì§€ë§Œ, ì‚¬ì‹¤ì„± ë¬¸ì œëŠ” ì—¬ì „íˆ ì¡´ì¬í•©ë‹ˆë‹¤. \n",
      "   - **ê·¼ê±° :** \"Despite obtaining lower factuality than retrieved knowledge, generated knowledge contributes more to the factuality of downstream tasks.\"\n",
      "\n",
      "ë‹µë³€ ìš”ì•½ \n",
      ": ìµœê·¼ ì—°êµ¬ë“¤ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ì—ì„œ ìƒë‹¹í•œ ì‚¬ì‹¤ì  ì§€ì‹ì„ í¬ì°©í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ê³  ìˆìœ¼ë©°, LLMì´ ìƒì„±í•œ ì§€ì‹ì€ ì •ë³´ ê²€ìƒ‰ ê¸°ìˆ ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤. LLMì˜ ë§¤ê°œë³€ìˆ˜ì— ì¸ì½”ë”©ëœ ì§€ì‹ì— ëŒ€í•œ ì—°êµ¬ëŠ” LLMì´ ìƒì„±í•˜ëŠ” ì§€ì‹ì˜ ì§ˆê³¼ ì‹ ë¢°ì„±ì„ í‰ê°€í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•˜ë©°, ìƒì„±ëœ ì§€ì‹ì€ í•˜ìœ„ ì‘ì—…ì—ì„œ ë” ìœ ìš©í•˜ê³  ê´€ë ¨ì„±ì´ ë†’ì§€ë§Œ ì‚¬ì‹¤ì„± ë¬¸ì œëŠ” ì—¬ì „íˆ ì¡´ì¬í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì—°êµ¬ ê²°ê³¼ëŠ” LLMì„ ì§€ì‹ ìƒì„±ê¸°ë¡œ í™œìš©í•˜ëŠ” ë° ìˆì–´ ì¤‘ìš”í•œ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "ì¸ìš© ë…¼ë¬¸ ì œëª© (Title): Language models as knowledge bases?\n",
      "\n",
      "í•´ë‹¹ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ì—ì„œ ì‚¬ì‹¤ì  ì§€ì‹ì„ ì–´ë–»ê²Œ íšë“í•˜ëŠ”ì§€ë¥¼ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•¨ìœ¼ë¡œì¨, LLMì˜ ì§€ì‹ ìƒì„± ëŠ¥ë ¥ì— ëŒ€í•œ ê¸°ì¡´ ì—°êµ¬ë¥¼ ë°œì „ì‹œì¼°ìŠµë‹ˆë‹¤. íŠ¹íˆ, LLMì´ ì •ë³´ ê²€ìƒ‰ ê¸°ìˆ ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ëŠ” ì´ìœ ë¥¼ ì„¤ëª…í•˜ê³ , ì§€ì‹ì˜ ì§ˆê³¼ ì‹ ë¢°ì„±ì„ í‰ê°€í•˜ëŠ” ë° ì¤‘ìš”í•œ ìš”ì†Œì¸ ì‚¬ì‹¤ì„± ë¬¸ì œë¥¼ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ, LLMì˜ ë§¤ê°œë³€ìˆ˜ì— ì¸ì½”ë”©ëœ ì§€ì‹ì˜ ë™ì  ë³€í™”ë¥¼ ë¶„ì„í•˜ì—¬, ì§€ì‹ì˜ íšë“ê³¼ ë§ê° ê³¼ì •ì´ LLMì˜ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê·œëª…í•¨ìœ¼ë¡œì¨, LLMì„ ì§€ì‹ ìƒì„±ê¸°ë¡œ í™œìš©í•˜ëŠ” ë° ìˆì–´ ì¤‘ìš”í•œ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì—°êµ¬ëŠ” LLMì˜ í›ˆë ¨ ë° í™œìš© ë°©ì•ˆì„ ê°œì„ í•˜ëŠ” ë° ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.\n",
      "####\n",
      "ì¸ìš© ë…¼ë¬¸ ì œëª© (Title): Extracting training data from large language models\n",
      "\n",
      "1. **ì§ˆë¬¸ :** ëª¨ë¸ì€ ëª¨ë“  íšë“ ê¹Šì´ì—ì„œ ë¡œê·¸ í™•ë¥ ì˜ ë” í° ê°œì„ ì„ ë³´ì´ì§€ë§Œ, ìŠì–´ë²„ë¦¬ëŠ” ì†ë„ê°€ ë” ë¹ ë¥´ë©°, ê²°êµ­ í›ˆë ¨ ì¢…ë£Œ ì‹œì (t = 2000)ì—ì„œ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ ì£¼ì… ì‹œë‚˜ë¦¬ì˜¤ì™€ ìœ ì‚¬í•œ ìˆ˜ì¤€ì˜ ê°œì„ ì„ ì´ˆë˜í•œë‹¤.  \n",
      "   - **ë‹µë³€ :** ëª¨ë¸ì€ í›ˆë ¨ ê³¼ì •ì—ì„œ ë¡œê·¸ í™•ë¥ ì´ ë” í¬ê²Œ ê°œì„ ë˜ì§€ë§Œ, ìŠì–´ë²„ë¦¬ëŠ” ì†ë„ê°€ ë¹¨ë¼ ê²°êµ­ í›ˆë ¨ ì¢…ë£Œ ì‹œì ì—ì„œ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ ì£¼ì… ì‹œë‚˜ë¦¬ì˜¤ì™€ ë¹„ìŠ·í•œ ê°œì„  ìˆ˜ì¤€ì— ë„ë‹¬í•œë‹¤. ì´ëŠ” ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì˜ íŠ¹ì • ì˜ˆì œë¥¼ ê¸°ì–µí•˜ëŠ” ë° í•œê³„ê°€ ìˆìŒì„ ë‚˜íƒ€ë‚¸ë‹¤.  \n",
      "   - **ê·¼ê±° :** \"the forgetting is faster, eventually resulting in a similar level of improvement at the end of the training (t = 2000) compared to the paraphrase injection scenario.\"\n",
      "\n",
      "2. **ì§ˆë¬¸ :** ê·¸ë¦¼ 5ì˜ ì¶”ì •ëœ x-ì ˆí¸ì€ í›ˆë ¨ì„ í†µí•´ íšë“í•œ ì‚¬ì‹¤ì  ì§€ì‹ì˜ ì™„ì „í•œ ì†ì‹¤ë¡œ ì´ì–´ì§ˆ ì¶”ê°€ í›ˆë ¨ í† í°ì˜ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.  \n",
      "   - **ë‹µë³€ :** ê·¸ë¦¼ 5ì˜ x-ì ˆí¸ì€ ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì—ì„œ ê¸°ì–µí•œ ì‚¬ì‹¤ì  ì§€ì‹ì´ ì™„ì „íˆ ì†Œì‹¤ë˜ê¸° ìœ„í•´ í•„ìš”í•œ ì¶”ê°€ í›ˆë ¨ í† í°ì˜ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ê¸°ì–µë ¥ê³¼ ê´€ë ¨ëœ ì¤‘ìš”í•œ ì§€í‘œë¡œ, íŠ¹ì • ì˜ˆì œê°€ ì–¼ë§ˆë‚˜ ìì£¼ ë“±ì¥í•˜ëŠ”ì§€ê°€ ë©”ëª¨ë¦¬ ìœ ì§€ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤.  \n",
      "   - **ê·¼ê±° :** \"the estimated x-intercepts in Figure 5 represent the number of additional training tokens that would lead to the complete loss of the factual knowledge acquired by training.\"\n",
      "\n",
      "3. **ì§ˆë¬¸ :** ì¤‘ë³µë˜ì§€ ì•Šì€ ë°ì´í„°ì™€ ë” í° ë°°ì¹˜ í¬ê¸°ë¡œ LLMì„ ì‚¬ì „ í›ˆë ¨í•˜ë©´ ì‚¬ì‹¤ì  ì§€ì‹ì˜ ìŠµë“ì´ í–¥ìƒë˜ì–´ í•™ìŠµí•œ ì‚¬ì‹¤ì  ì§€ì‹ì„ ìŠì–´ë²„ë¦¬ëŠ” ê²ƒì— ëŒ€í•´ ë” ê°•ë ¥í•´ì§„ë‹¤.  \n",
      "   - **ë‹µë³€ :** ì¤‘ë³µë˜ì§€ ì•Šì€ ë°ì´í„°ì™€ ë” í° ë°°ì¹˜ í¬ê¸°ë¡œ LLMì„ ì‚¬ì „ í›ˆë ¨í•˜ë©´ ëª¨ë¸ì´ ì‚¬ì‹¤ì  ì§€ì‹ì„ ë” ì˜ ìŠµë“í•˜ê²Œ ë˜ì–´, í•™ìŠµí•œ ì§€ì‹ì„ ìŠì–´ë²„ë¦¬ëŠ” ë° ë” ê°•ë ¥í•´ì§„ë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê³ , í›ˆë ¨ ë°ì´í„°ì˜ ë‹¤ì–‘ì„±ì„ ë†’ì´ëŠ” ë° ê¸°ì—¬í•œë‹¤.  \n",
      "   - **ê·¼ê±° :** \"Pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge.\"\n",
      "\n",
      "ë‹µë³€ ìš”ì•½: ì´ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì—ì„œ ê°œë³„ ì˜ˆì œë¥¼ ê¸°ì–µí•˜ê³  ì´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤€ë‹¤. ëª¨ë¸ì€ ë¡œê·¸ í™•ë¥ ì˜ ê°œì„ ì„ ë³´ì´ì§€ë§Œ, ìŠì–´ë²„ë¦¬ëŠ” ì†ë„ê°€ ë¹¨ë¼ ê²°êµ­ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì˜ ê°œì„ ì— ë„ë‹¬í•œë‹¤. ê·¸ë¦¼ 5ì˜ x-ì ˆí¸ì€ í›ˆë ¨ì„ í†µí•´ ìŠµë“í•œ ì‚¬ì‹¤ì  ì§€ì‹ì˜ ì†ì‹¤ì— í•„ìš”í•œ ì¶”ê°€ í›ˆë ¨ í† í° ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ë©°, ì¤‘ë³µë˜ì§€ ì•Šì€ ë°ì´í„°ì™€ í° ë°°ì¹˜ í¬ê¸°ë¡œ ì‚¬ì „ í›ˆë ¨ëœ LLMì€ ì‚¬ì‹¤ì  ì§€ì‹ì˜ ìŠµë“ì„ í–¥ìƒì‹œì¼œ ìŠì–´ë²„ë¦¬ëŠ” ê²ƒì— ëŒ€í•´ ë” ê°•ë ¥í•´ì§„ë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ í›ˆë ¨ ë° ë©”ëª¨ë¦¬ ìœ ì§€ì— ëŒ€í•œ ì¤‘ìš”í•œ í†µì°°ì„ ì œê³µí•˜ë©°, í–¥í›„ ì—°êµ¬ì—ì„œ ë©”ëª¨ë¦¬ì™€ ê´€ë ¨ëœ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ì ‘ê·¼ë²•ì´ í•„ìš”í•¨ì„ ì‹œì‚¬í•œë‹¤.\n",
      "\n",
      "ì¸ìš© ë…¼ë¬¸ ì œëª© (Title): Extracting training data from large language models\n",
      "\n",
      "í•´ë‹¹ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì´ í›ˆë ¨ ê³¼ì •ì—ì„œ ì‚¬ì‹¤ì  ì§€ì‹ì„ ì–´ë–»ê²Œ ìŠµë“í•˜ê³  ìŠì–´ë²„ë¦¬ëŠ”ì§€ë¥¼ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•¨ìœ¼ë¡œì¨, ê¸°ì¡´ ì—°êµ¬ì—ì„œ ì œê¸°ëœ ì§ˆë¬¸ë“¤ì— ëŒ€í•œ ëª…í™•í•œ ë‹µë³€ì„ ì œì‹œí•˜ì˜€ë‹¤. íŠ¹íˆ, ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì˜ íŠ¹ì • ì˜ˆì œë¥¼ ê¸°ì–µí•˜ëŠ” ë° í•œê³„ê°€ ìˆìŒì„ ë³´ì—¬ì£¼ë©°, ìŠì–´ë²„ë¦¬ëŠ” ì†ë„ê°€ ë¹ ë¥´ë‹¤ëŠ” ì ì„ ê°•ì¡°í•˜ì˜€ë‹¤. ë˜í•œ, ì¤‘ë³µë˜ì§€ ì•Šì€ ë°ì´í„°ì™€ í° ë°°ì¹˜ í¬ê¸°ë¡œ í›ˆë ¨í•  ê²½ìš° ì‚¬ì‹¤ì  ì§€ì‹ì˜ ìŠµë“ì´ í–¥ìƒë˜ê³  ìŠì–´ë²„ë¦¼ì— ëŒ€í•œ ì €í•­ë ¥ì´ ì¦ê°€í•œë‹¤ëŠ” ë°œê²¬ì€, LLMì˜ í›ˆë ¨ ë° ë©”ëª¨ë¦¬ ìœ ì§€ì— ëŒ€í•œ ì¤‘ìš”í•œ í†µì°°ì„ ì œê³µí•˜ë©°, í–¥í›„ ì—°êµ¬ ë°©í–¥ì— ëŒ€í•œ ê¸°ì´ˆë¥¼ ë§ˆë ¨í•˜ì˜€ë‹¤.\n",
      "####\n",
      "ì¸ìš© ë…¼ë¬¸ ì œëª© (Title): Large language models struggle to learn long-tail knowledge\n",
      "\n",
      "1. **ì§ˆë¬¸ :** ê¸´ ê¼¬ë¦¬ ì§€ì‹ì„ ìŠµë“í•˜ì§€ ëª»í•˜ëŠ” ì‹¤íŒ¨ì™€ ë°ì´í„°ì…‹ ì¤‘ë³µ ì œê±°ì˜ ì¤‘ìš”ì„±ì— ëŒ€í•´ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n",
      "   - **ë‹µë³€ :** ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ì¸í„°ë„·ì—ì„œ ë°©ëŒ€í•œ ì–‘ì˜ ì •ë³´ë¥¼ í•™ìŠµí•˜ì§€ë§Œ, íŠ¹ì • ì •ë³´ëŠ” ë“œë¬¼ê²Œ ë‚˜íƒ€ë‚˜ë©° ì´ëŸ¬í•œ ë“œë¬¸ ì •ë³´, ì¦‰ ê¸´ ê¼¬ë¦¬ ì§€ì‹ì„ ìŠµë“í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªìŠµë‹ˆë‹¤. ë°ì´í„°ì…‹ ì¤‘ë³µ ì œê±°ëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•˜ë©°, ëª¨ë¸ì´ ë‹¤ì–‘í•œ ì •ë³´ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤. ì—°êµ¬ì— ë”°ë¥´ë©´, ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ì—ì„œ ê´€ë ¨ ë¬¸ì„œì˜ ìˆ˜ì™€ ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ë³´ì…ë‹ˆë‹¤.\n",
      "   - **ê·¼ê±° :** \"ìš°ë¦¬ì˜ ê²°ê³¼ëŠ” ì •í™•ì„±ê³¼ ê´€ë ¨ ë¬¸ì„œ ìˆ˜ ê°„ì˜ ê°•í•œ ìƒê´€ê´€ê³„ì™€ ì¸ê³¼ ê´€ê³„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\"\n",
      "\n",
      "2. **ì§ˆë¬¸ :** ìµœê·¼ LLMì— ëŒ€í•œ ì—°êµ¬ì—ì„œ ê¸´ ê¼¬ë¦¬ ì§€ì‹ ìŠµë“ì´ ì €ì¡°í•˜ë‹¤ëŠ” ì‚¬ì‹¤ì´ ë°í˜€ì¡ŒìŠµë‹ˆë‹¤. ì´ì— ëŒ€í•œ ì„¤ëª…ì„ í•´ì£¼ì„¸ìš”.\n",
      "   - **ë‹µë³€ :** ìµœê·¼ ì—°êµ¬ì— ë”°ë¥´ë©´, LLMì€ ê¸´ ê¼¬ë¦¬ ì§€ì‹ì„ ìŠµë“í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìœ¼ë©°, ì´ëŠ” ëª¨ë¸ì´ ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ì—ì„œ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ì •ë³´ì— ì˜ì¡´í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ëª¨ë¸ì˜ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ ê¸´ ê¼¬ë¦¬ ì§€ì‹ í•™ìŠµ ëŠ¥ë ¥ì´ í–¥ìƒë˜ì§€ë§Œ, í˜„ì¬ì˜ ëª¨ë¸ì€ ê¸´ ê¼¬ë¦¬ ì§ˆë¬¸ì— ëŒ€í•´ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë‚´ê¸° ìœ„í•´ì„œëŠ” ìˆ˜ì‹­ ë°°ì˜ ê·œëª¨ë¡œ í™•ì¥í•´ì•¼ í•©ë‹ˆë‹¤.\n",
      "   - **ê·¼ê±° :** \"ìš°ë¦¬ëŠ” ì˜¤ëŠ˜ë‚ ì˜ ëª¨ë¸ì´ ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ì—ì„œ ì§€ì›ì´ ì ì€ ì§ˆë¬¸ì— ëŒ€í•´ ê²½ìŸë ¥ ìˆëŠ” QA ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ì„œëŠ” ìˆ˜ë§ì€ ë°°ìˆ˜ë¡œ í™•ì¥í•´ì•¼ í•œë‹¤ê³  ì¶”ì •í•©ë‹ˆë‹¤.\"\n",
      "\n",
      "ë‹µë³€ ìš”ì•½ \n",
      ": ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì€ ë°©ëŒ€í•œ ì–‘ì˜ ì •ë³´ë¥¼ í•™ìŠµí•˜ì§€ë§Œ, ë“œë¬¼ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ê¸´ ê¼¬ë¦¬ ì§€ì‹ì„ ìŠµë“í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ì—ì„œ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ì •ë³´ì— ì˜ì¡´í•˜ê¸° ë•Œë¬¸ì´ë©°, ë°ì´í„°ì…‹ ì¤‘ë³µ ì œê±°ê°€ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼, ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ê´€ë ¨ ë¬¸ì„œ ìˆ˜ì™€ ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ë³´ì´ë©°, ëª¨ë¸ì˜ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ ê¸´ ê¼¬ë¦¬ ì§€ì‹ í•™ìŠµ ëŠ¥ë ¥ì´ í–¥ìƒë˜ì§€ë§Œ, í˜„ì¬ì˜ ëª¨ë¸ì€ ê¸´ ê¼¬ë¦¬ ì§ˆë¬¸ì— ëŒ€í•´ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ì„ ë‚´ê¸° ìœ„í•´ì„œëŠ” ìˆ˜ì‹­ ë°°ì˜ ê·œëª¨ë¡œ í™•ì¥í•´ì•¼ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” LLMì˜ ì§€ì‹ ìŠµë“ ë°©ì‹ê³¼ ê´€ë ¨ëœ ì¤‘ìš”í•œ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "ì¸ìš© ë…¼ë¬¸ ì œëª© (Title): Large language models struggle to learn long-tail knowledge\n",
      "\n",
      "í•´ë‹¹ ë…¼ë¬¸ì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ê¸´ ê¼¬ë¦¬ ì§€ì‹ì„ ìŠµë“í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªëŠ” ì´ìœ ë¥¼ ë°ì´í„°ì…‹ì˜ ì¤‘ë³µì„±ê³¼ ê´€ë ¨í•˜ì—¬ ì„¤ëª…í•©ë‹ˆë‹¤. ì´ì™€ ê´€ë ¨í•˜ì—¬, ë³¸ ì—°êµ¬ëŠ” LLMì˜ ì‚¬ì‹¤ì  ì§€ì‹ ìŠµë“ ë©”ì»¤ë‹ˆì¦˜ì„ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬, ëª¨ë¸ì´ ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ì—ì„œ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ì •ë³´ì— ì˜ì¡´í•˜ëŠ” ê²½í–¥ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ë°ì´í„° ì¤‘ë³µ ì œê±°ê°€ ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í•¨ì„ ë³´ì—¬ì£¼ë©°, ëª¨ë¸ í¬ê¸°ì™€ í›ˆë ¨ ì¡°ê±´ì´ ì§€ì‹ ìŠµë“ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê·œëª…í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ í†µì°°ì€ LLMì˜ ì§€ì‹ ìŠµë“ ë°©ì‹ì— ëŒ€í•œ ì´í•´ë¥¼ ì‹¬í™”ì‹œí‚¤ê³ , ê¸´ ê¼¬ë¦¬ ì§€ì‹ ìŠµë“ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ ë°©í–¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.\n",
      "####\n"
     ]
    }
   ],
   "source": [
    "for index in related_reference.keys():\n",
    "    print(related_reference[index]['Summary'])\n",
    "    print()\n",
    "    print(related_reference[index]['Summary_QnA'])\n",
    "    print(\"####\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_summary = \"\"\n",
    "for i, index in enumerate(related_reference.keys()):\n",
    "    reference_summary += f\"{i+1}ë²ˆì§¸\\n\"+ related_reference[index]['Summary'] + \"\\n\\n\" + related_reference[index]['Summary_QnA'] + \"####\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 1. ê¸°ë³¸ ì •ë³´\n",
      "1) ì œëª©: How Do Large Language Models Acquire Factual Knowledge During Pretraining?\n",
      "2) ì €ì: Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, Minjoon Seo\n",
      "\n",
      "### 2. ì—°êµ¬ ëª©ì \n",
      "1) ë¬¸ì œì˜ì‹: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ì‚¬ì‹¤ì  ì§€ì‹ ìŠµë“ ë©”ì»¤ë‹ˆì¦˜\n",
      "2) ì„¤ëª…: ìµœê·¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ìƒë‹¹í•œ ì‚¬ì‹¤ì  ì§€ì‹ì„ ì €ì¥í•  ìˆ˜ ìˆë‹¤ëŠ” ê´€ì°°ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì´ë“¤ì´ ì‚¬ì „ í›ˆë ¨ ì¤‘ ì‚¬ì‹¤ì  ì§€ì‹ì„ ì–´ë–»ê²Œ ìŠµë“í•˜ëŠ”ì§€ì— ëŒ€í•œ ì´í•´ëŠ” ì œí•œì ì´ë‹¤. ë³¸ ì—°êµ¬ëŠ” LLMì˜ ì‚¬ì‹¤ì  ì§€ì‹ ìŠµë“ ê³¼ì •ì„ ë¶„ì„í•˜ì—¬, í›ˆë ¨ ë°ì´í„°ì˜ ì–‘, í›ˆë ¨ ë‹¨ê³„, ëª¨ë¸ í¬ê¸°, ë°°ì¹˜ í¬ê¸° ë“±ì˜ ë‹¤ì–‘í•œ ì¡°ê±´ì´ ì§€ì‹ ìŠµë“ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì¡°ì‚¬í•œë‹¤. ì´ë¥¼ í†µí•´ LLMì˜ í›ˆë ¨ ë™ì—­í•™ì„ ì´í•´í•˜ê³ , ì§€ì‹ ìŠµë“ì˜ ë©”ì»¤ë‹ˆì¦˜ì„ ê·œëª…í•˜ê³ ì í•œë‹¤.\n",
      "\n",
      "### 3. ì—°êµ¬ ë°©ë²•\n",
      "1) ì‹¤í—˜ ë°©ë²•: ì—°êµ¬ì§„ì€ LLMì˜ ì¤‘ê°„ ì‚¬ì „ í›ˆë ¨ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬, ì´ì „ì— ì ‘í•˜ì§€ ì•Šì€ ëª©í‘œ ì§€ì‹ì„ ì£¼ì…í•˜ê³ , ë‹¤ì–‘í•œ ì¡°ê±´ì—ì„œ ì‚¬ì‹¤ì  ì§€ì‹ì˜ ìŠµë“ ê³¼ì •ì„ ëª¨ë‹ˆí„°ë§í•˜ì˜€ë‹¤. \n",
      "2) ë°ì´í„°: FICTIONAL KNOWLEDGE ë°ì´í„°ì…‹ì„ êµ¬ì„±í•˜ì—¬, í—ˆêµ¬ì ì´ì§€ë§Œ í˜„ì‹¤ì ì¸ ê°œì²´ì— ëŒ€í•œ ì„¤ëª…ì„ í¬í•¨í•œ ë¬¸ì¥ì„ ì£¼ì…í•˜ì˜€ë‹¤. ì´ ë°ì´í„°ì…‹ì€ LLMì´ í›ˆë ¨ ì¤‘ì— ì ‘í•˜ì§€ ì•Šì€ ì§€ì‹ì„ í¬í•¨í•œë‹¤.\n",
      "3) ëª¨ë¸ ë° ë¶„ì„ ë°©ë²•: OLMo ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬, ì£¼ì…ëœ ì§€ì‹ì— ëŒ€í•œ ë¡œê·¸ í™•ë¥ ì„ í‰ê°€í•˜ê³ , ì§€ì‹ ìŠµë“ì˜ íš¨ê³¼ì„± ë° ìœ ì§€ ê°€ëŠ¥ì„±ì„ ì¸¡ì •í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ì„ ì •ì˜í•˜ì˜€ë‹¤.\n",
      "\n",
      "### 4. ì£¼ìš” ê²°ê³¼\n",
      "1) ì—°êµ¬ì˜ ì£¼ìš” ë°œê²¬: LLMì€ ì‚¬ì‹¤ì  ì§€ì‹ì„ ìŠµë“í•  ë•Œ, ë¯¸ì„¸í•œ í™•ë¥ ì˜ ëˆ„ì ì„ í†µí•´ ì´ë£¨ì–´ì§€ë©°, í›ˆë ¨ ì¤‘ ì§€ì‹ì´ ì£¼ì–´ì§€ì§€ ì•Šìœ¼ë©´ ìŠì–´ë²„ë¦¬ëŠ” ê²½í–¥ì´ ìˆë‹¤. ë˜í•œ, ëª¨ë¸ í¬ê¸°ê°€ í´ìˆ˜ë¡ ì§€ì‹ ìŠµë“ì˜ íš¨ê³¼ì„±ì´ ë†’ì•„ì§€ì§€ë§Œ, í›ˆë ¨ ë°ì´í„°ì˜ ì–‘ì´ ë§ì•„ì ¸ë„ íš¨ê³¼ì„±ì€ í¬ê²Œ ê°œì„ ë˜ì§€ ì•ŠëŠ”ë‹¤.\n",
      "2) ê¸°ì—¬ ë° ì„±ê³¼: ë³¸ ì—°êµ¬ëŠ” LLMì˜ ì‚¬ì‹¤ì  ì§€ì‹ ìŠµë“ ë™ì—­í•™ì„ ì„¸ë°€í•˜ê²Œ ë¶„ì„í•˜ê³ , ë°ì´í„° ì¤‘ë³µ ì œê±°ì™€ ë°°ì¹˜ í¬ê¸° ì¦ê°€ê°€ ì§€ì‹ ìŠµë“ì— ê¸ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ì ì„ ë°í˜€ë‚´ì–´, LLMì˜ í›ˆë ¨ ë°©ë²•ë¡ ì— ëŒ€í•œ ìƒˆë¡œìš´ í†µì°°ì„ ì œê³µí•œë‹¤.\n",
      "\n",
      "### 5. ê²°ë¡  ë° ì‹œì‚¬ì \n",
      "1) ê²°ë¡ : LLMì˜ ì‚¬ì‹¤ì  ì§€ì‹ ìŠµë“ì€ ë¯¸ì„¸í•œ í™•ë¥ ì˜ ëˆ„ì ì„ í†µí•´ ì´ë£¨ì–´ì§€ë©°, ìŠì–´ë²„ë¦¼ í˜„ìƒê³¼ì˜ ê´€ê³„ê°€ ì¤‘ìš”í•˜ë‹¤. \n",
      "2) ì‹œì‚¬ì : ì—°êµ¬ ê²°ê³¼ëŠ” LLMì˜ í›ˆë ¨ ë°ì´í„° êµ¬ì„± ë° í›ˆë ¨ ë°©ë²•ì— ëŒ€í•œ ì „ëµì  ì ‘ê·¼ì„ ì œì•ˆí•˜ë©°, LLMì˜ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬í•  ìˆ˜ ìˆë‹¤.\n",
      "3) ì—°êµ¬ì˜ í•œê³„: ë³¸ ì—°êµ¬ëŠ” íŠ¹ì • ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì— êµ­í•œë˜ì–´ ìˆìœ¼ë©°, ë‹¤ì–‘í•œ LLM ì•„í‚¤í…ì²˜ì— ëŒ€í•œ ì¼ë°˜í™” ê°€ëŠ¥ì„±ì— ëŒ€í•œ ê²€ì¦ì´ í•„ìš”í•˜ë‹¤.\n",
      "4) í–¥í›„ ì—°êµ¬ ë°©í–¥: LLMì˜ ì§€ì‹ ìŠµë“ ë©”ì»¤ë‹ˆì¦˜ì„ ë”ìš± ê¹Šì´ ì´í•´í•˜ê¸° ìœ„í•´, ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì§€ì‹ê³¼ í›ˆë ¨ ì¡°ê±´ì„ íƒìƒ‰í•˜ëŠ” ì¶”ê°€ ì—°êµ¬ê°€ í•„ìš”í•˜ë‹¤.\n"
     ]
    }
   ],
   "source": [
    "print(output['Basic_summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
