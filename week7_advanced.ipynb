{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path=\"config.env\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI()\n",
    "def message_to_openai(message):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        store=True,\n",
    "        messages=[\n",
    "            {\"role\":\"system\", \"content\":\"í•œêµ­ì–´ë¡œ 5ë¬¸ì¥ìœ¼ë¡œ ëŒ€ë‹µí•´ì£¼ì„¸ìš”.\"},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ]\n",
    "    )\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_terms = [\n",
    "    \"ì¸ê³µì§€ëŠ¥ (Artificial Intelligence, AI)\",\n",
    "    \"ë¨¸ì‹ ëŸ¬ë‹ (Machine Learning, ML)\",\n",
    "    \"ë”¥ëŸ¬ë‹ (Deep Learning)\",\n",
    "    \"ë°ì´í„° ë§ˆì´ë‹ (Data Mining)\",\n",
    "    \"ì§€ë„í•™ìŠµ (Supervised Learning)\",\n",
    "    \"ë¹„ì§€ë„í•™ìŠµ (Unsupervised Learning)\",\n",
    "    \"ê°•í™”í•™ìŠµ (Reinforcement Learning, RL)\",\n",
    "    \"ì „ì´í•™ìŠµ (Transfer Learning)\",\n",
    "    \"ìê¸°ì§€ë„í•™ìŠµ (Self-Supervised Learning)\",\n",
    "    \"ìƒì„± ëª¨ë¸ (Generative Model)\",\n",
    "    \"í™•ë¥  ë¶„í¬ (Probability Distribution)\",\n",
    "    \"ë² ì´ì¦ˆ ì •ë¦¬ (Bayes' Theorem)\",\n",
    "    \"ì„ í˜•ëŒ€ìˆ˜ (Linear Algebra)\",\n",
    "    \"í–‰ë ¬ ì—°ì‚° (Matrix Operations)\",\n",
    "    \"ë¯¸ë¶„ (Differentiation)\",\n",
    "    \"í¸ë¯¸ë¶„ (Partial Derivative)\",\n",
    "    \"ê²½ì‚¬ í•˜ê°•ë²• (Gradient Descent)\",\n",
    "    \"ì†ì‹¤ í•¨ìˆ˜ (Loss Function)\",\n",
    "    \"ë¹„ìš© í•¨ìˆ˜ (Cost Function)\",\n",
    "    \"ìµœì í™” (Optimization)\",\n",
    "    \"ë‰´ëŸ° (Neuron)\",\n",
    "    \"í¼ì…‰íŠ¸ë¡  (Perceptron)\",\n",
    "    \"ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  (Multilayer Perceptron, MLP)\",\n",
    "    \"í•©ì„±ê³± ì‹ ê²½ë§ (Convolutional Neural Network, CNN)\",\n",
    "    \"ìˆœí™˜ ì‹ ê²½ë§ (Recurrent Neural Network, RNN)\",\n",
    "    \"ì¥ë‹¨ê¸° ê¸°ì–µ ë„¤íŠ¸ì›Œí¬ (Long Short-Term Memory, LSTM)\",\n",
    "    \"ê²Œì´íŠ¸ ìˆœí™˜ ìœ ë‹› (Gated Recurrent Unit, GRU)\",\n",
    "    \"íŠ¸ëœìŠ¤í¬ë¨¸ (Transformer)\",\n",
    "    \"ì¸ì½”ë”-ë””ì½”ë” ëª¨ë¸ (Encoder-Decoder Model)\",\n",
    "    \"BERT (Bidirectional Encoder Representations from Transformers)\",\n",
    "    \"í™œì„±í™” í•¨ìˆ˜ (Activation Function)\",\n",
    "    \"ReLU (Rectified Linear Unit)\",\n",
    "    \"ì‹œê·¸ëª¨ì´ë“œ (Sigmoid)\",\n",
    "    \"ì†Œí”„íŠ¸ë§¥ìŠ¤ (Softmax)\",\n",
    "    \"ë°°ì¹˜ ì •ê·œí™” (Batch Normalization)\",\n",
    "    \"ë“œë¡­ì•„ì›ƒ (Dropout)\",\n",
    "    \"í•™ìŠµë¥  (Learning Rate)\",\n",
    "    \"ê³¼ì í•© (Overfitting)\",\n",
    "    \"ë°ì´í„° ì¦ê°• (Data Augmentation)\",\n",
    "    \"ëª¨ë¸ ì •ê·œí™” (Model Regularization)\",\n",
    "    \"í† í°í™” (Tokenization)\",\n",
    "    \"ì„ë² ë”© (Embedding)\",\n",
    "    \"ì›Œë“œíˆ¬ë²¡ (Word2Vec)\",\n",
    "    \"ê¸€ë¡œë¸Œ (GloVe)\",\n",
    "    \"ë¬¸ì¥í”¼ìŠ¤ (SentencePiece)\",\n",
    "    \"BPE (Byte Pair Encoding)\",\n",
    "    \"ì–¸ì–´ ëª¨ë¸ (Language Model)\",\n",
    "    \"ê°ì„± ë¶„ì„ (Sentiment Analysis)\",\n",
    "    \"ê°œì²´ëª… ì¸ì‹ (Named Entity Recognition, NER)\",\n",
    "    \"ê¸°ê³„ ë²ˆì—­ (Machine Translation)\",\n",
    "    \"ì´ë¯¸ì§€ ë¶„ë¥˜ (Image Classification)\",\n",
    "    \"ê°ì²´ íƒì§€ (Object Detection)\",\n",
    "    \"ì„¸ê·¸ë©˜í…Œì´ì…˜ (Segmentation)\",\n",
    "    \"í•©ì„±ê³± (Convolution)\",\n",
    "    \"í’€ë§ (Pooling)\",\n",
    "    \"YOLO (You Only Look Once)\",\n",
    "    \"ResNet (Residual Network)\",\n",
    "    \"VGGNet (Visual Geometry Group Network)\",\n",
    "    \"GAN (Generative Adversarial Network)\",\n",
    "    \"Autoencoder (ìë™ ì¸ì½”ë”)\",\n",
    "    \"í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²• (Stochastic Gradient Descent, SGD)\",\n",
    "    \"Adam ì˜µí‹°ë§ˆì´ì € (Adam Optimizer)\",\n",
    "    \"RMSprop\",\n",
    "    \"ëª¨ë©˜í…€ (Momentum)\",\n",
    "    \"í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (Hyperparameter Tuning)\",\n",
    "    \"ì´ˆê¸°í™” ë°©ë²• (Weight Initialization)\",\n",
    "    \"ì „ì—­ ìµœì í™” (Global Optimization)\",\n",
    "    \"êµ°ì§‘ ë¶„ì„ (Clustering)\",\n",
    "    \"ì•™ìƒë¸” í•™ìŠµ (Ensemble Learning)\",\n",
    "    \"ëœë¤ í¬ë ˆìŠ¤íŠ¸ (Random Forest)\",\n",
    "    \"ì •í™•ë„ (Accuracy)\",\n",
    "    \"ì •ë°€ë„ (Precision)\",\n",
    "    \"ì¬í˜„ìœ¨ (Recall)\",\n",
    "    \"F1 ì ìˆ˜ (F1 Score)\",\n",
    "    \"AUC-ROC ê³¡ì„  (AUC-ROC Curve)\",\n",
    "    \"í‰ê·  ì œê³± ì˜¤ì°¨ (Mean Squared Error, MSE)\",\n",
    "    \"êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ (Cross-Entropy Loss)\",\n",
    "    \"BLEU ì ìˆ˜ (BLEU Score)\",\n",
    "    \"ROUGE ì ìˆ˜ (ROUGE Score)\",\n",
    "    \"í˜¼ë™ í–‰ë ¬ (Confusion Matrix)\",\n",
    "    \"ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ (Large Language Model, LLM)\",\n",
    "    \"ChatGPT\",\n",
    "    \"ì¸ê³µì§€ëŠ¥ ìœ¤ë¦¬ (AI Ethics)\",\n",
    "    \"ì—°í•© í•™ìŠµ (Federated Learning)\",\n",
    "    \"ì§€ì† í•™ìŠµ (Continual Learning)\",\n",
    "    \"íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ (Foundation Model)\",\n",
    "    \"ë¯¸ì„¸ ì¡°ì • (Fine-Tuning)\",\n",
    "    \"ì œë¡œìƒ· í•™ìŠµ (Zero-Shot Learning)\",\n",
    "    \"ì†Œìˆ˜ìƒ· í•™ìŠµ (Few-Shot Learning)\",\n",
    "    \"ì²´ì¸ ì˜¤ë¸Œ ë ë¡œìš°íŠ¸ (Chain-of-Thought)\",\n",
    "    \"í´ë¼ìš°ë“œ AI (Cloud AI)\",\n",
    "    \"API ê¸°ë°˜ AI ì„œë¹„ìŠ¤ (AI-as-a-Service)\",\n",
    "    \"ë°ì´í„° ë ˆì´ë¸”ë§ (Data Labeling)\",\n",
    "    \"MLOps (Machine Learning Operations)\",\n",
    "    \"Explainable AI (XAI, ì„¤ëª… ê°€ëŠ¥í•œ AI)\",\n",
    "    \"ì‹œê³„ì—´ ë¶„ì„ (Time Series Analysis)\",\n",
    "    \"ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬ (TensorFlow, PyTorch)\",\n",
    "    \"ONNX (Open Neural Network Exchange)\",\n",
    "    \"AutoML (ìë™í™” ë¨¸ì‹ ëŸ¬ë‹)\",\n",
    "    \"AI ëª¨ë¸ ë°°í¬ (AI Model Deployment)\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [07:21<00:00,  4.41s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "prompt_template = \"{term}ì´ ë¬´ì—‡ì¸ì§€ ì„¤ëª…í•´ì¤˜\"\n",
    "result =[]\n",
    "for item in tqdm(ai_terms):\n",
    "    ai_terms_dict = {\"instruction\": None, \"input\":\"\", \"output\":None}\n",
    "    prompt = prompt_template.format(term=item)\n",
    "    response = message_to_openai(prompt)\n",
    "    ai_terms_dict['instruction'] = prompt\n",
    "    ai_terms_dict[\"output\"] = response.choices[0].message.content\n",
    "    result.append(ai_terms_dict)\n",
    "\n",
    "# JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "with open(\"corpus.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# JSON ë°ì´í„° ë¡œë“œ\n",
    "dataset = load_dataset(\"json\", data_files=\"corpus.json\")[\"train\"]\n",
    "\n",
    "# ë°ì´í„° ë¶„í•  (70% Train, 20% Validation, 10% Test)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "val_dataset = dataset.select(range(train_size, train_size + val_size))\n",
    "test_dataset = dataset.select(range(train_size + val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e7f6493129406d9d531d39f2908d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2122c8736a44c3bb6aababac1936d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4797bd6176a04df98f9d4e94cb8f7616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd5cc5a3d3e47ca8cf10cf7e375cfdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1021ef9d6e418cba26016432fb56bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "395487e0db8545b98f2bcb0f83887df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # LLaMAëŠ” ê¸°ë³¸ì ìœ¼ë¡œ pad_tokenì´ ì—†ìœ¼ë¯€ë¡œ eos_token ì‚¬ìš©\n",
    "\n",
    "# ë°ì´í„° í¬ë§· ë³€í™˜ í•¨ìˆ˜\n",
    "def format_llama_prompt(example):\n",
    "    prompt = f\"### ì§ˆë¬¸: {example['instruction']}\\n### ë‹µë³€: {example['output']}\"\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# í† í¬ë‚˜ì´ì§• í•¨ìˆ˜\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# Train, Validation, Test ë°ì´í„°ì…‹ ë³€í™˜\n",
    "train_dataset = train_dataset.map(format_llama_prompt).map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(format_llama_prompt).map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(format_llama_prompt).map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paradeigma/workspace/python/sparta_coding/.env/lib/python3.9/site-packages/transformers/training_args.py:1573: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_24461/2366907953.py:26: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "/home/paradeigma/workspace/python/sparta_coding/.env/lib/python3.9/site-packages/transformers/training_args.py:1573: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 35\u001b[0m\n\u001b[1;32m     26\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m     27\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     28\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# ëª¨ë¸ í•™ìŠµ ì‹œì‘\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/python/sparta_coding/.env/lib/python3.9/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/python/sparta_coding/.env/lib/python3.9/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/workspace/python/sparta_coding/.env/lib/python3.9/site-packages/transformers/trainer.py:3713\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3711\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m-> 3713\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3715\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/workspace/python/sparta_coding/.env/lib/python3.9/site-packages/accelerate/accelerator.py:2244\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2243\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2244\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m~/workspace/python/sparta_coding/.env/lib/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/python/sparta_coding/.env/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/python/sparta_coding/.env/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# TrainingArguments ì„¤ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama-3.2-1B-sft\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=30,\n",
    "    eval_steps=30,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Mixed Precision ì‚¬ìš© (GPU í™˜ê²½)\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"wandb\", \n",
    "    run_name=\"llama-3.2-1B-finetuning\",  \n",
    ")\n",
    "\n",
    "# SFTTrainer ìƒì„±\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ ì‹œì‘\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
